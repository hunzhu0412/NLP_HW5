{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=\"ERROR\")\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path\n",
    "output_dir = \"nmt\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Split('train'): ['newscommentary_v14',\n",
      "                  'wikititles_v1',\n",
      "                  'uncorpus_v1',\n",
      "                  'casia2015',\n",
      "                  'casict2011',\n",
      "                  'casict2015',\n",
      "                  'datum2015',\n",
      "                  'datum2017',\n",
      "                  'neu2017'],\n",
      " Split('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(tmp_builder.subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在設定檔 config 裡頭指定新聞評論這個資料來源並請 TensorFlow Datasets 下載\n",
    "config = tfds.translate.wmt.WmtConfig(\n",
    "  version=tfds.core.Version('0.0.3'),\n",
    "  language_pair=(\"zh\", \"en\"),\n",
    "  subsets={\n",
    "    tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "  }\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n",
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "examples = builder.as_dataset(split=['train[:20%]','train[20%:21%]','train[21%:]'], as_supervised=True)\n",
    "\n",
    "train_examples, val_examples, _ = examples\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'The fear is real and visceral, and politicians ignore it at their peril.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word \\xe2\\x80\\x9cliberal\\xe2\\x80\\x9d \\xe2\\x80\\x93 a champion of the cause of individual freedom.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe4\\xba\\x8b\\xe5\\xae\\x9e\\xe4\\xb8\\x8a\\xef\\xbc\\x8c\\xe5\\xbe\\xb7\\xe5\\x9b\\xbd\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xb1\\x80\\xe5\\x8a\\xbf\\xe9\\x9c\\x80\\xe8\\xa6\\x81\\xe7\\x9a\\x84\\xe4\\xb8\\x8d\\xe8\\xbf\\x87\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe7\\xac\\xa6\\xe5\\x90\\x88\\xe7\\xbe\\x8e\\xe5\\x9b\\xbd\\xe6\\x89\\x80\\xe8\\xb0\\x93\\xe2\\x80\\x9c\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe2\\x80\\x9d\\xe5\\xae\\x9a\\xe4\\xb9\\x89\\xe7\\x9a\\x84\\xe7\\x9c\\x9f\\xe6\\xad\\xa3\\xe7\\x9a\\x84\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe5\\x85\\x9a\\xe6\\xb4\\xbe\\xef\\xbc\\x8c\\xe4\\xb9\\x9f\\xe5\\xb0\\xb1\\xe6\\x98\\xaf\\xe4\\xb8\\xaa\\xe4\\xba\\xba\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe4\\xba\\x8b\\xe4\\xb8\\x9a\\xe7\\x9a\\x84\\xe5\\x80\\xa1\\xe5\\xaf\\xbc\\xe8\\x80\\x85\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xbf\\x85\\xe9\\xa1\\xbb\\xe4\\xbb\\x98\\xe5\\x87\\xba\\xe5\\xb7\\xa8\\xe5\\xa4\\xa7\\xe7\\x9a\\x84\\xe5\\x8a\\xaa\\xe5\\x8a\\x9b\\xe5\\x92\\x8c\\xe5\\x9f\\xba\\xe7\\xa1\\x80\\xe8\\xae\\xbe\\xe6\\x96\\xbd\\xe6\\x8a\\x95\\xe8\\xb5\\x84\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\xae\\x8c\\xe6\\x88\\x90\\xe5\\x90\\x91\\xe5\\x8f\\xaf\\xe5\\x86\\x8d\\xe7\\x94\\x9f\\xe8\\x83\\xbd\\xe6\\xba\\x90\\xe7\\x9a\\x84\\xe8\\xbf\\x87\\xe6\\xb8\\xa1\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for en, zh in train_examples.take(3):\n",
    "  print(en)\n",
    "  print(zh)\n",
    "  print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fear is real and visceral, and politicians ignore it at their peril.\n",
      "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
      "----------\n",
      "In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word “liberal” – a champion of the cause of individual freedom.\n",
      "事实上，德国政治局势需要的不过是一个符合美国所谓“自由”定义的真正的自由党派，也就是个人自由事业的倡导者。\n",
      "----------\n",
      "Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.\n",
      "必须付出巨大的努力和基础设施投资才能完成向可再生能源的过渡。\n",
      "----------\n",
      "In this sense, it is critical to recognize the fundamental difference between “urban villages” and their rural counterparts.\n",
      "在这方面，关键在于认识到“城市村落”和农村村落之间的根本区别。\n",
      "----------\n",
      "A strong European voice, such as Nicolas Sarkozy’s during the French presidency of the EU, may make a difference, but only for six months, and at the cost of reinforcing other European countries’ nationalist feelings in reaction to the expression of “Gallic pride.”\n",
      "法国担任轮值主席国期间尼古拉·萨科奇统一的欧洲声音可能让人耳目一新，但这种声音却只持续了短短六个月，而且付出了让其他欧洲国家在面对“高卢人的骄傲”时民族主义情感进一步被激发的代价。\n",
      "----------\n",
      "Most of Japan’s bondholders are nationals (if not the central bank) and have an interest in political stability.\n",
      "日本债券持有人大多为本国国民（甚至中央银行 ） ， 政治稳定符合他们的利益。\n",
      "----------\n",
      "Paul Romer, one of the originators of new growth theory, has accused some leading names, including the Nobel laureate Robert Lucas, of what he calls “mathiness” – using math to obfuscate rather than clarify.\n",
      "新增长理论创始人之一的保罗·罗默（Paul Romer）也批评一些著名经济学家，包括诺贝尔奖获得者罗伯特·卢卡斯（Robert Lucas）在内，说他们“数学性 ” （ 罗默的用语）太重，结果是让问题变得更加模糊而不是更加清晰。\n",
      "----------\n",
      "It is, in fact, a capsule depiction of the United States Federal Reserve and the European Central Bank.\n",
      "事实上，这就是对美联储和欧洲央行的简略描述。\n",
      "----------\n",
      "Given these variables, the degree to which migration is affected by asylum-seekers will not be easy to predict or control.\n",
      "考虑到这些变量，移民受寻求庇护者的影响程度很难预测或控制。\n",
      "----------\n",
      "WASHINGTON, DC – In the 2016 American presidential election, Hillary Clinton and Donald Trump agreed that the US economy is suffering from dilapidated infrastructure, and both called for greater investment in renovating and upgrading the country’s public capital stock.\n",
      "华盛顿—在2016年美国总统选举中，希拉里·克林顿和唐纳德·特朗普都认为美国经济饱受基础设施陈旧的拖累，两人都要求加大投资用于修缮和升级美国公共资本存量。\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "  en = en_t.numpy().decode(\"utf-8\")\n",
    "  zh = zh_t.numpy().decode(\"utf-8\")\n",
    "  \n",
    "  print(en)\n",
    "  print(zh)\n",
    "  print('-' * 10)\n",
    "  \n",
    "  # 之後用來簡單評估模型的訓練情況\n",
    "  sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：8113\n",
      "前 10 個 subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'is_', 'that_']\n",
      "\n",
      "CPU times: user 1min 27s, sys: 5.92 s, total: 1min 33s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#建立中文與英文字典\n",
    "try:\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "    print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (en.numpy() for en, _ in train_examples), \n",
    "      target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    "  \n",
    "    # 將字典檔案存下以方便下次 warmstart\n",
    "    subword_encoder_en.save_to_file(en_vocab_file)\n",
    "  \n",
    "\n",
    "print(f\"字典大小：{subword_encoder_en.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_en.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125, 5202, 7889, 523, 6062, 7903]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = 'I love my family.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Subword\n",
      "---------------\n",
      "  125     I \n",
      " 5202     love\n",
      " 7889      \n",
      "  523     my \n",
      " 6062     family\n",
      " 7903     .\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:10}{1:6}\".format(\"Index\", \"Subword\"))\n",
    "print(\"-\" * 15)\n",
    "for idx in indices:\n",
    "    subword = subword_encoder_en.decode([idx])\n",
    "    print('{0:5}{1:6}'.format(idx, ' ' * 5 + subword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I love my family.', 'I love my family.')\n"
     ]
    }
   ],
   "source": [
    "indices = subword_encoder_en.encode(sample_string)\n",
    "decoded_string = subword_encoder_en.decode(indices)\n",
    "assert decoded_string == sample_string\n",
    "pprint((sample_string, decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：4205\n",
      "前 10 個 subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\n",
      "\n",
      "CPU times: user 7min 23s, sys: 9.01 s, total: 7min 32s\n",
      "Wall time: 8min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#建立中文字典\n",
    "try:\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "    print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (zh.numpy() for _, zh in train_examples), \n",
    "      target_vocab_size=2**13, # 有需要可以調整字典大小\n",
    "      max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "\n",
    "    # 將字典檔案存下以方便下次 warmstart \n",
    "    subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f\"字典大小：{subword_encoder_zh.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[英中原文]（轉換前）\n",
      "The eurozone’s collapse forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "\n",
      "--------------------\n",
      "\n",
      "[英中序列]（轉換後）\n",
      "[16, 900, 11, 6, 1527, 874, 8, 230, 2259, 2728, 239, 3, 89, 1236, 7903]\n",
      "[44, 202, 168, 1, 852, 201, 231, 592, 44, 87, 17, 124, 106, 38, 7, 279, 86, 18, 212, 265, 3]\n"
     ]
    }
   ],
   "source": [
    "en = \"The eurozone’s collapse forces a major realignment of European politics.\"\n",
    "zh = \"欧元区的瓦解强迫欧洲政治进行一次重大改组。\"\n",
    "\n",
    "# 將文字轉成為 subword indices\n",
    "en_indices = subword_encoder_en.encode(en)\n",
    "zh_indices = subword_encoder_zh.encode(zh)\n",
    "\n",
    "print(\"[英中原文]（轉換前）\")\n",
    "print(en)\n",
    "print(zh)\n",
    "print()\n",
    "print('-' * 20)\n",
    "print()\n",
    "print(\"[英中序列]（轉換後）\")\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_t, zh_t):\n",
    "  # 因為字典的索引從 0 開始，\n",
    "  # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "  # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "  en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "  # 同理，不過是使用中文字典的最後一個索引 + 1\n",
    "  zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "  return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 BOS 的 index： 8113\n",
      "英文 EOS 的 index： 8114\n",
      "中文 BOS 的 index： 4205\n",
      "中文 EOS 的 index： 4206\n",
      "\n",
      "輸入為 2 個 Tensors：\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'The fear is real and visceral, and politicians ignore it at their peril.'>,\n",
      " <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82'>)\n",
      "---------------\n",
      "輸出為 2 個索引序列：\n",
      "([8113,\n",
      "  16,\n",
      "  1284,\n",
      "  9,\n",
      "  243,\n",
      "  5,\n",
      "  1275,\n",
      "  1756,\n",
      "  156,\n",
      "  1,\n",
      "  5,\n",
      "  1016,\n",
      "  5566,\n",
      "  21,\n",
      "  38,\n",
      "  33,\n",
      "  2982,\n",
      "  7965,\n",
      "  7903,\n",
      "  8114],\n",
      " [4205,\n",
      "  10,\n",
      "  151,\n",
      "  574,\n",
      "  1298,\n",
      "  6,\n",
      "  374,\n",
      "  55,\n",
      "  29,\n",
      "  193,\n",
      "  5,\n",
      "  1,\n",
      "  3,\n",
      "  3981,\n",
      "  931,\n",
      "  431,\n",
      "  125,\n",
      "  1,\n",
      "  17,\n",
      "  124,\n",
      "  33,\n",
      "  20,\n",
      "  97,\n",
      "  1089,\n",
      "  1247,\n",
      "  861,\n",
      "  3,\n",
      "  4206])\n"
     ]
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "print('英文 BOS 的 index：', subword_encoder_en.vocab_size)\n",
    "print('英文 EOS 的 index：', subword_encoder_en.vocab_size + 1)\n",
    "print('中文 BOS 的 index：', subword_encoder_zh.vocab_size)\n",
    "print('中文 EOS 的 index：', subword_encoder_zh.vocab_size + 1)\n",
    "\n",
    "print('\\n輸入為 2 個 Tensors：')\n",
    "pprint((en_t, zh_t))\n",
    "print('-' * 15)\n",
    "print('輸出為 2 個索引序列：')\n",
    "pprint((en_indices, zh_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[8113   16 1284    9  243    5 1275 1756  156    1    5 1016 5566   21\n",
      "   38   33 2982 7965 7903 8114], shape=(20,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[4205   10  151  574 1298    6  374   55   29  193    5    1    3 3981\n",
      "  931  431  125    1   17  124   33   20   97 1089 1247  861    3 4206], shape=(28,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def tf_encode(en_t, zh_t):\n",
    "  # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors\n",
    "  # 要到 `tf.py_funtion` 裡頭才是\n",
    "  # 另外因為索引都是整數，所以使用 `tf.int64`\n",
    "  return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "# `tmp_dataset` 為說明用資料集，說明完所有重要的 func，\n",
    "# 我們會從頭建立一個正式的 `train_dataset`\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "  # en, zh 分別代表英文與中文的索引序列\n",
    "  return tf.logical_and(tf.size(en) <= max_length,\n",
    "                        tf.size(zh) <= max_length)\n",
    "\n",
    "# tf.data.Dataset.filter(func) 只會回傳 func 為真的例子\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有英文與中文序列長度都不超過 40 個 tokens\n",
      "訓練資料集裡總共有 29784 筆數據\n"
     ]
    }
   ],
   "source": [
    "# 因為我們數據量小可以這樣 count\n",
    "num_examples = 0\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "  cond1 = len(en_indices) <= MAX_LENGTH\n",
    "  cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "  assert cond1 and cond2\n",
    "  num_examples += 1\n",
    "\n",
    "print(f\"所有英文與中文序列長度都不超過 {MAX_LENGTH} 個 tokens\")\n",
    "print(f\"訓練資料集裡總共有 {num_examples} 筆數據\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113   16 1284 ...    0    0    0]\n",
      " [8113 1894 1302 ...    0    0    0]\n",
      " [8113   44   40 ...    0    0    0]\n",
      " ...\n",
      " [8113  122  506 ...    0    0    0]\n",
      " [8113   16  215 ...    0    0    0]\n",
      " [8113 7443 7889 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205   10  151 ...    0    0    0]\n",
      " [4205  206  275 ...    0    0    0]\n",
      " [4205    5   10 ...    0    0    0]\n",
      " ...\n",
      " [4205   34    6 ...    0    0    0]\n",
      " [4205  317  256 ...    0    0    0]\n",
      " [4205  167  326 ...    0    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 將 batch 裡的所有序列都 pad 到同樣長度\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "# 訓練集\n",
    "train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                 .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                 .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                 .cache() # 加快讀取數據\n",
    "                 .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                 .padded_batch(BATCH_SIZE, # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "# 驗證集\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113 3093    1 ...    0    0    0]\n",
      " [8113 2478 2396 ...    0    0    0]\n",
      " [8113 1063 1066 ...    0    0    0]\n",
      " ...\n",
      " [8113  122   12 ...    0    0    0]\n",
      " [8113 3563  775 ...    0    0    0]\n",
      " [8113   41    2 ...    0    0    0]], shape=(128, 37), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205    9  717 ...    0    0    0]\n",
      " [4205    4   33 ...    0    0    0]\n",
      " [4205   71   86 ...    0    0    0]\n",
      " ...\n",
      " [4205   29   62 ...    0    0    0]\n",
      " [4205  104  112 ...    0    0    0]\n",
      " [4205   34    6 ...    3 4206    0]], shape=(128, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It is important.', '这很重要。'),\n",
      " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "pprint(demo_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print('' * 10)\n",
    "print('tar:', tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       " array([[[-0.00560867,  0.04747799, -0.02269538,  0.02303809],\n",
       "         [ 0.02988834,  0.00620367,  0.03613586, -0.02179264],\n",
       "         [ 0.01707463,  0.03969477,  0.02194964, -0.03702695],\n",
       "         [ 0.01390957,  0.0195005 ,  0.03259501,  0.02743271],\n",
       "         [ 0.02233395, -0.04510158, -0.00677161,  0.04184886],\n",
       "         [-0.00289515, -0.02603846, -0.04859028,  0.00019344],\n",
       "         [-0.02180409, -0.00259377,  0.01643033, -0.01761655],\n",
       "         [-0.02180409, -0.00259377,  0.01643033, -0.01761655]],\n",
       " \n",
       "        [[-0.00560867,  0.04747799, -0.02269538,  0.02303809],\n",
       "         [ 0.00088333,  0.02109436,  0.0129119 , -0.03082365],\n",
       "         [ 0.01852404,  0.04630077, -0.02833566, -0.02577083],\n",
       "         [ 0.04514705,  0.03137792, -0.04057758,  0.04044752],\n",
       "         [ 0.04274544,  0.02157653, -0.04907006,  0.02949495],\n",
       "         [-0.02063758, -0.04873876, -0.03854288,  0.020351  ],\n",
       "         [ 0.02233395, -0.04510158, -0.00677161,  0.04184886],\n",
       "         [-0.00289515, -0.02603846, -0.04859028,  0.00019344]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       " array([[[ 0.04993161,  0.02566061, -0.0146607 ,  0.00574071],\n",
       "         [ 0.01592635, -0.02025861,  0.00020757, -0.04203112],\n",
       "         [ 0.03664401,  0.00267266,  0.01699981, -0.04485391],\n",
       "         [ 0.02957764,  0.02617563, -0.01678916,  0.00045334],\n",
       "         [-0.03879989, -0.01194926,  0.03493711, -0.04563121],\n",
       "         [-0.01690708,  0.03204078,  0.04683626,  0.02910907],\n",
       "         [-0.02911331,  0.02794905,  0.01279471, -0.03680716],\n",
       "         [ 0.04868478, -0.0262701 ,  0.02982837,  0.04889109],\n",
       "         [ 0.04868478, -0.0262701 ,  0.02982837,  0.04889109],\n",
       "         [ 0.04868478, -0.0262701 ,  0.02982837,  0.04889109]],\n",
       " \n",
       "        [[ 0.04993161,  0.02566061, -0.0146607 ,  0.00574071],\n",
       "         [ 0.00005664, -0.03074064,  0.04639968,  0.0047685 ],\n",
       "         [ 0.03511107,  0.04525493, -0.03638245, -0.01101079],\n",
       "         [-0.04905901,  0.03428625,  0.00740141, -0.01450055],\n",
       "         [ 0.02496466,  0.03609515,  0.0253484 , -0.03687034],\n",
       "         [ 0.01206124, -0.02820102,  0.03362811, -0.04972026],\n",
       "         [ 0.00390694,  0.03242769,  0.02957014, -0.0285221 ],\n",
       "         [-0.00034374,  0.00425357,  0.00814558, -0.02010503],\n",
       "         [-0.01690708,  0.03204078,  0.04683626,  0.02910907],\n",
       "         [-0.02911331,  0.02794905,  0.01279471, -0.03680716]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar[0]: tf.Tensor([0 0 0], shape=(3,), dtype=int64)\n",
      "--------------------\n",
      "emb_tar[0]: tf.Tensor(\n",
      "[[ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      " [ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      " [ 0.04868478 -0.0262701   0.02982837  0.04889109]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"tar[0]:\", tar[0][-3:])\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_tar[0]:\", emb_tar[0][-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遮罩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "tf.squeeze(inp_mask): tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tf.squeeze(inp_mask):\", tf.squeeze(inp_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\n",
    "tf.random.set_seed(9527)\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "  # 將 `q`、 `k` 做點積再 scale\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "  # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # 以注意權重對 v 做加權平均（weighted average）\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[0.37517118 0.37485787 0.3749141  0.49988133]\n",
      "  [0.3748806  0.37499705 0.3749746  0.50012225]\n",
      "  [0.37480643 0.3749749  0.3748127  0.5001838 ]\n",
      "  [0.37515733 0.37485746 0.37509224 0.49988168]\n",
      "  [0.37525243 0.3749971  0.37524804 0.49981332]\n",
      "  [0.37502667 0.3750993  0.37499174 0.49998993]\n",
      "  [0.37485623 0.37504378 0.3749714  0.50007737]\n",
      "  [0.37485623 0.37504378 0.3749714  0.50007737]]\n",
      "\n",
      " [[0.6251291  0.24996676 0.6249881  0.37510455]\n",
      "  [0.6251714  0.24987376 0.6250084  0.3749109 ]\n",
      "  [0.6251849  0.24989976 0.6249447  0.37497264]\n",
      "  [0.62492216 0.25006002 0.62483686 0.37510434]\n",
      "  [0.6248967  0.2500843  0.62485194 0.37509596]\n",
      "  [0.6247307  0.25023276 0.6250605  0.3751313 ]\n",
      "  [0.6246762  0.2501855  0.62492657 0.375069  ]\n",
      "  [0.6248294  0.25016233 0.6250129  0.375083  ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "attention_weights: tf.Tensor(\n",
      "[[[0.12520449 0.12492143 0.12502347 0.12504236 0.12492433 0.12498906\n",
      "   0.12494739 0.12494739]\n",
      "  [0.12490316 0.12514745 0.12512518 0.12504764 0.12492979 0.1248524\n",
      "   0.12499719 0.12499719]\n",
      "  [0.12500356 0.12512356 0.12520888 0.12502067 0.1247822  0.12484152\n",
      "   0.12500982 0.12500982]\n",
      "  [0.12502603 0.12504958 0.12502423 0.12512913 0.12500218 0.12484696\n",
      "   0.12496093 0.12496093]\n",
      "  [0.1249385  0.12496223 0.12481622 0.1250327  0.12528123 0.12510075\n",
      "   0.12493414 0.12493414]\n",
      "  [0.12501831 0.12489989 0.12489061 0.1248925  0.12511584 0.125216\n",
      "   0.12498342 0.12498342]\n",
      "  [0.12495127 0.12501934 0.12503359 0.12498114 0.12492385 0.12495804\n",
      "   0.1250664  0.1250664 ]\n",
      "  [0.12495127 0.12501934 0.12503359 0.12498114 0.12492385 0.12495804\n",
      "   0.1250664  0.1250664 ]]\n",
      "\n",
      " [[0.12513778 0.12492909 0.1250635  0.12512262 0.12509063 0.12487613\n",
      "   0.12485777 0.12492246]\n",
      "  [0.12503712 0.12513526 0.12512644 0.12497072 0.12497191 0.12490185\n",
      "   0.12489326 0.12496345]\n",
      "  [0.12507287 0.1250277  0.12518612 0.12508868 0.12509021 0.12480955\n",
      "   0.12477899 0.12494591]\n",
      "  [0.1250443  0.1247846  0.125001   0.12524554 0.12521334 0.12484669\n",
      "   0.12494876 0.12491573]\n",
      "  [0.12501165 0.12478511 0.12500185 0.12521265 0.12519886 0.12488545\n",
      "   0.12494731 0.12495708]\n",
      "  [0.12489854 0.12481634 0.12482258 0.1249473  0.12498678 0.12524597\n",
      "   0.1251301  0.12515235]\n",
      "  [0.12488351 0.12481108 0.12479535 0.12505281 0.12505202 0.12513345\n",
      "   0.1252261  0.1250457 ]\n",
      "  [0.12492067 0.1248537  0.12493474 0.12499218 0.12503423 0.1251281\n",
      "   0.12501812 0.12511821]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"output:\", output)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "inp_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "inp_mask = create_padding_mask(inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_mask:\", inp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[0.16691592 0.16653857 0.16667458 0.1666998  0.16654243 0.16662873\n",
      "   0.         0.        ]\n",
      "  [0.16653629 0.166862   0.16683231 0.16672891 0.1665718  0.1664686\n",
      "   0.         0.        ]\n",
      "  [0.16667578 0.16683577 0.16694954 0.16669858 0.16638063 0.16645971\n",
      "   0.         0.        ]\n",
      "  [0.166684   0.1667154  0.1666816  0.16682146 0.1666522  0.16644527\n",
      "   0.         0.        ]\n",
      "  [0.16655542 0.16658705 0.16639242 0.16668099 0.16701232 0.16677172\n",
      "   0.         0.        ]\n",
      "  [0.16668372 0.16652583 0.16651346 0.16651598 0.16681376 0.1669473\n",
      "   0.         0.        ]\n",
      "  [0.1666312  0.16672198 0.16674098 0.16667102 0.16659464 0.16664024\n",
      "   0.         0.        ]\n",
      "  [0.1666312  0.16672198 0.16674098 0.16667102 0.16659464 0.16664024\n",
      "   0.         0.        ]]\n",
      "\n",
      " [[0.12513778 0.12492909 0.1250635  0.12512262 0.12509063 0.12487613\n",
      "   0.12485777 0.12492246]\n",
      "  [0.12503712 0.12513526 0.12512644 0.12497072 0.12497191 0.12490185\n",
      "   0.12489326 0.12496345]\n",
      "  [0.12507287 0.1250277  0.12518612 0.12508868 0.12509021 0.12480955\n",
      "   0.12477899 0.12494591]\n",
      "  [0.1250443  0.1247846  0.125001   0.12524554 0.12521334 0.12484669\n",
      "   0.12494876 0.12491573]\n",
      "  [0.12501165 0.12478511 0.12500185 0.12521265 0.12519886 0.12488545\n",
      "   0.12494731 0.12495708]\n",
      "  [0.12489854 0.12481634 0.12482258 0.1249473  0.12498678 0.12524597\n",
      "   0.1251301  0.12515235]\n",
      "  [0.12488351 0.12481108 0.12479535 0.12505281 0.12505202 0.12513345\n",
      "   0.1252261  0.1250457 ]\n",
      "  [0.12492067 0.1248537  0.12493474 0.12499218 0.12503423 0.1251281\n",
      "   0.12501812 0.12511821]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 這次讓我們將 padding mask 放入注意函式並觀察\n",
    "# 注意權重的變化\n",
    "mask = tf.squeeze(inp_mask, axis=1) # (batch_size, 1, seq_len_q)\n",
    "_, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 2), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.12485777, 0.12492246],\n",
       "        [0.12489326, 0.12496345],\n",
       "        [0.12477899, 0.12494591],\n",
       "        [0.12494876, 0.12491573],\n",
       "        [0.12494731, 0.12495708],\n",
       "        [0.1251301 , 0.12515235],\n",
       "        [0.1252261 , 0.1250457 ],\n",
       "        [0.12501812, 0.12511821]]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事實上也不完全是上句話的翻譯，\n",
    "# 因為我們在第一個維度還是把兩個句子都拿出來方便你比較\n",
    "attention_weights[:, :, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.04993161  0.02566061 -0.0146607   0.00574071]\n",
      "  [ 0.01592635 -0.02025861  0.00020757 -0.04203112]\n",
      "  [ 0.03664401  0.00267266  0.01699981 -0.04485391]\n",
      "  [ 0.02957764  0.02617563 -0.01678916  0.00045334]\n",
      "  [-0.03879989 -0.01194926  0.03493711 -0.04563121]\n",
      "  [-0.01690708  0.03204078  0.04683626  0.02910907]\n",
      "  [-0.02911331  0.02794905  0.01279471 -0.03680716]\n",
      "  [ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      "  [ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      "  [ 0.04868478 -0.0262701   0.02982837  0.04889109]]\n",
      "\n",
      " [[ 0.04993161  0.02566061 -0.0146607   0.00574071]\n",
      "  [ 0.00005664 -0.03074064  0.04639968  0.0047685 ]\n",
      "  [ 0.03511107  0.04525493 -0.03638245 -0.01101079]\n",
      "  [-0.04905901  0.03428625  0.00740141 -0.01450055]\n",
      "  [ 0.02496466  0.03609515  0.0253484  -0.03687034]\n",
      "  [ 0.01206124 -0.02820102  0.03362811 -0.04972026]\n",
      "  [ 0.00390694  0.03242769  0.02957014 -0.0285221 ]\n",
      "  [-0.00034374  0.00425357  0.00814558 -0.02010503]\n",
      "  [-0.01690708  0.03204078  0.04683626  0.02910907]\n",
      "  [-0.02911331  0.02794905  0.01279471 -0.03680716]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "# 其遮罩為一個右上角的三角形\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask\", look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49970007 0.5003     0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.33315077 0.33332184 0.33352733 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.25014326 0.24983339 0.24994957 0.25007382 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.19964628 0.20010275 0.20006648 0.19974118 0.20044337 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16661058 0.16647813 0.16656879 0.1666195  0.16670413 0.16701886\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14270584 0.14282398 0.14284945 0.1427611  0.14299566 0.14285225\n",
      "   0.14301173 0.         0.         0.        ]\n",
      "  [0.12508176 0.12493542 0.12498348 0.12499884 0.12480913 0.12505396\n",
      "   0.12475887 0.12537858 0.         0.        ]\n",
      "  [0.11114639 0.11101635 0.11105905 0.1110727  0.11090413 0.11112168\n",
      "   0.11085947 0.11141013 0.11141013 0.        ]\n",
      "  [0.10000483 0.09988783 0.09992625 0.09993853 0.09978686 0.0999826\n",
      "   0.09974667 0.10024214 0.10024214 0.10024214]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49943003 0.50056994 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3336204  0.33253548 0.33384418 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24974783 0.2498701  0.24993283 0.2504492  0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.19999613 0.19982648 0.20003657 0.19991072 0.20023012 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16651267 0.16677019 0.16646032 0.16653857 0.16675167 0.1669665\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14278138 0.14276777 0.14281087 0.14286155 0.14296992 0.14286101\n",
      "   0.14294744 0.         0.         0.        ]\n",
      "  [0.12495741 0.12497579 0.12497292 0.12499849 0.12503463 0.12503818\n",
      "   0.12502576 0.12499688 0.         0.        ]\n",
      "  [0.11103684 0.11114057 0.11100205 0.11116983 0.11111403 0.11101251\n",
      "   0.11115178 0.11106349 0.11130893 0.        ]\n",
      "  [0.09988347 0.09991807 0.09994931 0.10009097 0.10003836 0.09999622\n",
      "   0.10005122 0.09998883 0.09998595 0.10009758]]], shape=(2, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 讓我們用目標語言（中文）的 batch\n",
    "# 來模擬 Decoder 處理的情況\n",
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0.49970007, 0.5003    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.49943003, 0.50056994, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[[-0.00560867  0.04747799 -0.02269538  0.02303809]\n",
      "  [ 0.02988834  0.00620367  0.03613586 -0.02179264]\n",
      "  [ 0.01707463  0.03969477  0.02194964 -0.03702695]\n",
      "  [ 0.01390957  0.0195005   0.03259501  0.02743271]\n",
      "  [ 0.02233395 -0.04510158 -0.00677161  0.04184886]\n",
      "  [-0.00289515 -0.02603846 -0.04859028  0.00019344]\n",
      "  [-0.02180409 -0.00259377  0.01643033 -0.01761655]\n",
      "  [-0.02180409 -0.00259377  0.01643033 -0.01761655]]\n",
      "\n",
      " [[-0.00560867  0.04747799 -0.02269538  0.02303809]\n",
      "  [ 0.00088333  0.02109436  0.0129119  -0.03082365]\n",
      "  [ 0.01852404  0.04630077 -0.02833566 -0.02577083]\n",
      "  [ 0.04514705  0.03137792 -0.04057758  0.04044752]\n",
      "  [ 0.04274544  0.02157653 -0.04907006  0.02949495]\n",
      "  [-0.02063758 -0.04873876 -0.03854288  0.020351  ]\n",
      "  [ 0.02233395 -0.04510158 -0.00677161  0.04184886]\n",
      "  [-0.00289515 -0.02603846 -0.04859028  0.00019344]]], shape=(2, 8, 4), dtype=float32)\n",
      "output: tf.Tensor(\n",
      "[[[[-0.00560867  0.04747799]\n",
      "   [ 0.02988834  0.00620367]\n",
      "   [ 0.01707463  0.03969477]\n",
      "   [ 0.01390957  0.0195005 ]\n",
      "   [ 0.02233395 -0.04510158]\n",
      "   [-0.00289515 -0.02603846]\n",
      "   [-0.02180409 -0.00259377]\n",
      "   [-0.02180409 -0.00259377]]\n",
      "\n",
      "  [[-0.02269538  0.02303809]\n",
      "   [ 0.03613586 -0.02179264]\n",
      "   [ 0.02194964 -0.03702695]\n",
      "   [ 0.03259501  0.02743271]\n",
      "   [-0.00677161  0.04184886]\n",
      "   [-0.04859028  0.00019344]\n",
      "   [ 0.01643033 -0.01761655]\n",
      "   [ 0.01643033 -0.01761655]]]\n",
      "\n",
      "\n",
      " [[[-0.00560867  0.04747799]\n",
      "   [ 0.00088333  0.02109436]\n",
      "   [ 0.01852404  0.04630077]\n",
      "   [ 0.04514705  0.03137792]\n",
      "   [ 0.04274544  0.02157653]\n",
      "   [-0.02063758 -0.04873876]\n",
      "   [ 0.02233395 -0.04510158]\n",
      "   [-0.00289515 -0.02603846]]\n",
      "\n",
      "  [[-0.02269538  0.02303809]\n",
      "   [ 0.0129119  -0.03082365]\n",
      "   [-0.02833566 -0.02577083]\n",
      "   [-0.04057758  0.04044752]\n",
      "   [-0.04907006  0.02949495]\n",
      "   [-0.03854288  0.020351  ]\n",
      "   [-0.00677161  0.04184886]\n",
      "   [-0.04859028  0.00019344]]]], shape=(2, 2, 8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "  # x.shape: (batch_size, seq_len, d_model)\n",
    "  batch_size = tf.shape(x)[0]\n",
    "  \n",
    "  # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "  assert d_model % num_heads == 0\n",
    "  depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "  \n",
    "  # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "  # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "  # (batch_size, seq_len, num_heads, depth)\n",
    "  reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "  \n",
    "  # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "  # (batch_size, num_heads, seq_len, depth)\n",
    "  output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "d_model = 4\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)  \n",
    "print(\"x:\", x)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # 在初始的時候建立一些必要參數\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "    self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "    \n",
    "    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n",
    "    \n",
    "    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n",
    "    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n",
    "  \n",
    "  # 這跟我們前面看過的函式有 87% 相似\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n",
    "    # 輸出會多一個 head 維度\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model)) \n",
    "    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    # 通過最後一個線性轉換\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 4\n",
      "num_heads: 2\n",
      "\n",
      "q.shape: (2, 8, 4)\n",
      "k.shape: (2, 8, 4)\n",
      "v.shape: (2, 8, 4)\n",
      "padding_mask.shape: (2, 1, 1, 8)\n",
      "output.shape: (2, 8, 4)\n",
      "attention_weights.shape: (2, 2, 8, 8)\n",
      "\n",
      "output: tf.Tensor(\n",
      "[[[-0.00327943  0.00399235  0.00838342  0.00491207]\n",
      "  [-0.00327759  0.00398509  0.00836597  0.0049063 ]\n",
      "  [-0.00329186  0.0040026   0.00837681  0.00490017]\n",
      "  [-0.00325883  0.00396902  0.0083676   0.00492077]\n",
      "  [-0.00325683  0.0039606   0.00836151  0.00492136]\n",
      "  [-0.00328912  0.00399488  0.00838186  0.00490919]\n",
      "  [-0.00328153  0.00399106  0.00838646  0.00492062]\n",
      "  [-0.00328153  0.00399106  0.00838646  0.00492062]]\n",
      "\n",
      " [[ 0.00704482 -0.00709887 -0.01162914 -0.00506988]\n",
      "  [ 0.0070353  -0.00708863 -0.01162999 -0.00507903]\n",
      "  [ 0.00701774 -0.00706921 -0.01162845 -0.00509827]\n",
      "  [ 0.00704594 -0.00710293 -0.01163476 -0.00507246]\n",
      "  [ 0.00704145 -0.00709804 -0.01163448 -0.00507723]\n",
      "  [ 0.00706659 -0.00712551 -0.0116346  -0.00504973]\n",
      "  [ 0.00707867 -0.00714073 -0.01163985 -0.00503906]\n",
      "  [ 0.00704626 -0.00710269 -0.01163294 -0.00507078]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# emb_inp.shape == (batch_size, seq_len, d_model)\n",
    "#               == (2, 8, 4)\n",
    "assert d_model == emb_inp.shape[-1]  == 4\n",
    "num_heads = 2\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\\n\")\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 簡單將 v, k, q 都設置為 `emb_inp`\n",
    "# 順便看看 padding mask 的作用。\n",
    "# 別忘記，第一個英文序列的最後兩個 tokens 是 <pad>\n",
    "v = k = q = emb_inp\n",
    "padding_mask = create_padding_mask(inp)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"padding_mask.shape:\", padding_mask.shape)\n",
    "\n",
    "output, attention_weights = mha(v, k, q, mask)\n",
    "print(\"output.shape:\", output.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)\n",
    "\n",
    "print(\"\\noutput:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "  # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 10, 512)\n",
      "out.shape: (64, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "out = ffn(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"out.shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4 # FFN 的輸入輸出張量的最後一維皆為 `d_model`\n",
    "dff = 6\n",
    "\n",
    "# 建立一個小 FFN\n",
    "small_ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "# 懂子詞梗的站出來\n",
    "dummy_sentence = tf.constant([[5, 5, 6, 6], \n",
    "                              [5, 5, 6, 6], \n",
    "                              [9, 5, 2, 7], \n",
    "                              [9, 5, 2, 7],\n",
    "                              [9, 5, 2, 7]], dtype=tf.float32)\n",
    "small_ffn(dummy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  # Transformer 論文內預設 dropout rate 為 0.1\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 一樣，一個 sub-layer 一個 dropout layer\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n",
    "  def call(self, x, training, mask):\n",
    "    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n",
    "    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n",
    "    \n",
    "    # sub-layer 1: MHA\n",
    "    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n",
    "    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n",
    "    attn_output, attn = self.mha(x, x, x, mask)  \n",
    "    attn_output = self.dropout1(attn_output, training=training) \n",
    "    out1 = self.layernorm1(x + attn_output)  \n",
    "    \n",
    "    # sub-layer 2: FFN\n",
    "    ffn_output = self.ffn(out1) \n",
    "    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "    out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "emb_inp: tf.Tensor(\n",
      "[[[-0.00560867  0.04747799 -0.02269538  0.02303809]\n",
      "  [ 0.02988834  0.00620367  0.03613586 -0.02179264]\n",
      "  [ 0.01707463  0.03969477  0.02194964 -0.03702695]\n",
      "  [ 0.01390957  0.0195005   0.03259501  0.02743271]\n",
      "  [ 0.02233395 -0.04510158 -0.00677161  0.04184886]\n",
      "  [-0.00289515 -0.02603846 -0.04859028  0.00019344]\n",
      "  [-0.02180409 -0.00259377  0.01643033 -0.01761655]\n",
      "  [-0.02180409 -0.00259377  0.01643033 -0.01761655]]\n",
      "\n",
      " [[-0.00560867  0.04747799 -0.02269538  0.02303809]\n",
      "  [ 0.00088333  0.02109436  0.0129119  -0.03082365]\n",
      "  [ 0.01852404  0.04630077 -0.02833566 -0.02577083]\n",
      "  [ 0.04514705  0.03137792 -0.04057758  0.04044752]\n",
      "  [ 0.04274544  0.02157653 -0.04907006  0.02949495]\n",
      "  [-0.02063758 -0.04873876 -0.03854288  0.020351  ]\n",
      "  [ 0.02233395 -0.04510158 -0.00677161  0.04184886]\n",
      "  [-0.00289515 -0.02603846 -0.04859028  0.00019344]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.86216056  1.4637611  -0.98385155  0.38225093]\n",
      "  [ 1.3363163   0.07108469  0.07694276 -1.4843438 ]\n",
      "  [ 0.59806234  1.3321484  -0.9159351  -1.0142757 ]\n",
      "  [-1.0543758  -0.722393    1.5160675   0.26070133]\n",
      "  [ 0.9189819  -1.4025555  -0.49012303  0.9736966 ]\n",
      "  [ 0.8219302  -0.07099353 -1.6078302   0.8568936 ]\n",
      "  [-0.65529084  0.5421367   1.3353008  -1.2221467 ]\n",
      "  [-0.65529084  0.5421367   1.3353008  -1.2221467 ]]\n",
      "\n",
      " [[-1.0159998   1.4832687  -0.8074313   0.34016246]\n",
      "  [-0.2817186   1.7040187  -0.65882176 -0.76347834]\n",
      "  [-0.14133535  1.4905005  -1.325853   -0.02331202]\n",
      "  [-0.32045436  1.2569025  -1.4375111   0.5010631 ]\n",
      "  [-0.17393968  1.2225792  -1.5054464   0.45680696]\n",
      "  [-0.59407175 -0.23042391 -0.8633068   1.6878026 ]\n",
      "  [ 0.60704947 -1.315686   -0.55380434  1.2624408 ]\n",
      "  [-0.17528476  1.0175374  -1.5567303   0.7144778 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 之後可以調的超參數。這邊為了 demo 設小一點\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# 新建一個使用上述參數的 Encoder Layer\n",
    "enc_layer = EncoderLayer(d_model, num_heads, dff)\n",
    "padding_mask = create_padding_mask(inp)  # 建立一個當前輸入 batch 使用的 padding mask\n",
    "enc_out = enc_layer(emb_inp, training=False, mask=padding_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"padding_mask:\", padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_inp:\", emb_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "assert emb_inp.shape == enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 裡頭會有 N 個 DecoderLayer，\n",
    "# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    # 3 個 sub-layers 的主角們\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    # 定義每個 sub-layer 用的 LayerNorm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 定義每個 sub-layer 用的 Dropout\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "    # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n",
    "    # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n",
    "    # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "    # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "    # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n",
    "    # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n",
    "    # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n",
    "    # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 10), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask: tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_padding_mask:\", tar_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask:\", look_ahead_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.04993161  0.02566061 -0.0146607   0.00574071]\n",
      "  [ 0.01592635 -0.02025861  0.00020757 -0.04203112]\n",
      "  [ 0.03664401  0.00267266  0.01699981 -0.04485391]\n",
      "  [ 0.02957764  0.02617563 -0.01678916  0.00045334]\n",
      "  [-0.03879989 -0.01194926  0.03493711 -0.04563121]\n",
      "  [-0.01690708  0.03204078  0.04683626  0.02910907]\n",
      "  [-0.02911331  0.02794905  0.01279471 -0.03680716]\n",
      "  [ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      "  [ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      "  [ 0.04868478 -0.0262701   0.02982837  0.04889109]]\n",
      "\n",
      " [[ 0.04993161  0.02566061 -0.0146607   0.00574071]\n",
      "  [ 0.00005664 -0.03074064  0.04639968  0.0047685 ]\n",
      "  [ 0.03511107  0.04525493 -0.03638245 -0.01101079]\n",
      "  [-0.04905901  0.03428625  0.00740141 -0.01450055]\n",
      "  [ 0.02496466  0.03609515  0.0253484  -0.03687034]\n",
      "  [ 0.01206124 -0.02820102  0.03362811 -0.04972026]\n",
      "  [ 0.00390694  0.03242769  0.02957014 -0.0285221 ]\n",
      "  [-0.00034374  0.00425357  0.00814558 -0.02010503]\n",
      "  [-0.01690708  0.03204078  0.04683626  0.02910907]\n",
      "  [-0.02911331  0.02794905  0.01279471 -0.03680716]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.86216056  1.4637611  -0.98385155  0.38225093]\n",
      "  [ 1.3363163   0.07108469  0.07694276 -1.4843438 ]\n",
      "  [ 0.59806234  1.3321484  -0.9159351  -1.0142757 ]\n",
      "  [-1.0543758  -0.722393    1.5160675   0.26070133]\n",
      "  [ 0.9189819  -1.4025555  -0.49012303  0.9736966 ]\n",
      "  [ 0.8219302  -0.07099353 -1.6078302   0.8568936 ]\n",
      "  [-0.65529084  0.5421367   1.3353008  -1.2221467 ]\n",
      "  [-0.65529084  0.5421367   1.3353008  -1.2221467 ]]\n",
      "\n",
      " [[-1.0159998   1.4832687  -0.8074313   0.34016246]\n",
      "  [-0.2817186   1.7040187  -0.65882176 -0.76347834]\n",
      "  [-0.14133535  1.4905005  -1.325853   -0.02331202]\n",
      "  [-0.32045436  1.2569025  -1.4375111   0.5010631 ]\n",
      "  [-0.17393968  1.2225792  -1.5054464   0.45680696]\n",
      "  [-0.59407175 -0.23042391 -0.8633068   1.6878026 ]\n",
      "  [ 0.60704947 -1.315686   -0.55380434  1.2624408 ]\n",
      "  [-0.17528476  1.0175374  -1.5567303   0.7144778 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[ 0.8132313   1.0897974  -1.3639673  -0.5390614 ]\n",
      "  [ 0.8691148  -1.3257993   1.0597903  -0.6031058 ]\n",
      "  [ 0.9549857  -0.97287357  1.0436794  -1.0257915 ]\n",
      "  [ 0.42379266  1.3658564  -1.3151882  -0.47446084]\n",
      "  [-0.8028193  -0.41508734  1.7138362  -0.49592948]\n",
      "  [-1.1326958  -0.46220163  1.5821515   0.01274579]\n",
      "  [-0.8015562   0.74464023  1.2119687  -1.1550527 ]\n",
      "  [ 0.37784606 -1.4110677  -0.29958287  1.3328044 ]\n",
      "  [ 0.37784606 -1.4110675  -0.29958293  1.3328046 ]\n",
      "  [ 0.37784606 -1.4110675  -0.29958293  1.3328046 ]]\n",
      "\n",
      " [[ 1.0835966   0.5703038  -1.5795712  -0.07432926]\n",
      "  [-0.04565383 -1.5857912   0.5397041   1.091741  ]\n",
      "  [ 0.64291024  0.82474256 -1.690242    0.22258916]\n",
      "  [-1.4495696   1.1850512   0.6115359  -0.3470175 ]\n",
      "  [ 1.2564776   0.40511647 -0.16871892 -1.4928752 ]\n",
      "  [ 0.8572839  -1.6969562   0.5480493   0.2916231 ]\n",
      "  [-0.30363944  0.22089958  1.4298187  -1.347079  ]\n",
      "  [-0.07896468 -1.491244    1.3033589   0.2668498 ]\n",
      "  [-1.4414071  -0.34291682  0.56593806  1.2183857 ]\n",
      "  [-1.0289103   0.9556911   1.0429182  -0.9696989 ]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_self_attn_weights.shape: (2, 2, 10, 10)\n",
      "dec_enc_attn_weights: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "dec_layer = DecoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "# 來源、目標語言的序列都需要 padding mask\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\n",
    "dec_out, dec_self_attn_weights, dec_enc_attn_weights = dec_layer(\n",
    "    emb_tar, enc_out, False, combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_out:\", dec_out)\n",
    "assert emb_tar.shape == dec_out.shape\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_self_attn_weights.shape:\", dec_self_attn_weights.shape)\n",
    "print(\"dec_enc_attn_weights:\", dec_enc_attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 512), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.84147096,  0.8218562 ,  0.8019618 , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.9092974 ,  0.9364147 ,  0.95814437, ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        ...,\n",
       "        [ 0.12357312,  0.97718984, -0.24295525, ...,  0.9999863 ,\n",
       "          0.99998724,  0.99998814],\n",
       "        [-0.76825464,  0.7312359 ,  0.63279754, ...,  0.9999857 ,\n",
       "          0.9999867 ,  0.9999876 ],\n",
       "        [-0.95375264, -0.14402692,  0.99899054, ...,  0.9999851 ,\n",
       "          0.9999861 ,  0.9999871 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以下直接參考 TensorFlow 官方 tutorial \n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  sines = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  cosines = np.cos(angle_rads[:, 1::2])\n",
    "  \n",
    "  pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "  \n",
    "  pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "d_model = 512\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  # Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了：\n",
    "  # - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "  # - input_vocab_size: 用來把索引轉成詞嵌入向量\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "    # 建立 `num_layers` 個 EncoderLayers\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "    # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "    # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model)\n",
    "    input_seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model)\n",
    "    # 再加上對應長度的位置編碼\n",
    "    x = self.embedding(x)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "    # 對 embedding 跟位置編碼的總合做 regularization\n",
    "    # 這在 Decoder 也會做\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    # 通過 N 個 EncoderLayer 做編碼\n",
    "    for i, enc_layer in enumerate(self.enc_layers):\n",
    "      x = enc_layer(x, training, mask)\n",
    "      # 以下只是用來 demo EncoderLayer outputs\n",
    "      #print('-' * 20)\n",
    "      #print(f\"EncoderLayer {i + 1}'s output:\", x)\n",
    "      \n",
    "    \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.78493303 -0.59196836 -0.33270508  1.7096066 ]\n",
      "  [-0.50706553 -0.5110135  -0.70823187  1.7263108 ]\n",
      "  [-0.39270186 -0.03102653 -1.1583618   1.5820903 ]\n",
      "  [-0.5561631   0.38050264 -1.2407897   1.41645   ]\n",
      "  [-0.9043198   0.19381055 -0.847289    1.5577981 ]\n",
      "  [-0.9732156  -0.2299279  -0.46524605  1.6683894 ]\n",
      "  [-0.8468199  -0.543447   -0.31013632  1.7004032 ]\n",
      "  [-0.62432784 -0.56790483 -0.53900087  1.7312335 ]]\n",
      "\n",
      " [[-0.7742377  -0.60764706 -0.3280062   1.709891  ]\n",
      "  [-0.47978228 -0.56156075 -0.6860291   1.727372  ]\n",
      "  [-0.30068302 -0.07366982 -1.197396    1.5717487 ]\n",
      "  [-0.51478416  0.2787244  -1.2290851   1.4651448 ]\n",
      "  [-0.8963448   0.26754576 -0.895411    1.52421   ]\n",
      "  [-0.9755364  -0.22618692 -0.46569633  1.6674197 ]\n",
      "  [-0.8760044  -0.5448399  -0.2709955   1.6918397 ]\n",
      "  [-0.6013044  -0.59936666 -0.5306773   1.7313484 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Encoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "enc_out = encoder(inp, training=False, mask=None)\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  # 初始參數跟 Encoder 只差在用 `target_vocab_size` 而非 `inp_vocab_size`\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    # 為中文（目標語言）建立詞嵌入層\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "  \n",
    "  # 呼叫時的參數跟 DecoderLayer 一模一樣\n",
    "  def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "    \n",
    "    tar_seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}  # 用來存放每個 Decoder layer 的注意權重\n",
    "    \n",
    "    # 這邊跟 Encoder 做的事情完全一樣\n",
    "    x = self.embedding(x)  # (batch_size, tar_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :tar_seq_len, :]\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    \n",
    "    for i, dec_layer in enumerate(self.dec_layers):\n",
    "      x, block1, block2 = dec_layer(x, enc_output, training,\n",
    "                                    combined_mask, inp_padding_mask)\n",
    "      \n",
    "      # 將從每個 Decoder layer 取得的注意權重全部存下來回傳，方便我們觀察\n",
    "      attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, tar_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.78493303 -0.59196836 -0.33270508  1.7096066 ]\n",
      "  [-0.50706553 -0.5110135  -0.70823187  1.7263108 ]\n",
      "  [-0.39270186 -0.03102653 -1.1583618   1.5820903 ]\n",
      "  [-0.5561631   0.38050264 -1.2407897   1.41645   ]\n",
      "  [-0.9043198   0.19381055 -0.847289    1.5577981 ]\n",
      "  [-0.9732156  -0.2299279  -0.46524605  1.6683894 ]\n",
      "  [-0.8468199  -0.543447   -0.31013632  1.7004032 ]\n",
      "  [-0.62432784 -0.56790483 -0.53900087  1.7312335 ]]\n",
      "\n",
      " [[-0.7742377  -0.60764706 -0.3280062   1.709891  ]\n",
      "  [-0.47978228 -0.56156075 -0.6860291   1.727372  ]\n",
      "  [-0.30068302 -0.07366982 -1.197396    1.5717487 ]\n",
      "  [-0.51478416  0.2787244  -1.2290851   1.4651448 ]\n",
      "  [-0.8963448   0.26754576 -0.895411    1.52421   ]\n",
      "  [-0.9755364  -0.22618692 -0.46569633  1.6674197 ]\n",
      "  [-0.8760044  -0.5448399  -0.2709955   1.6918397 ]\n",
      "  [-0.6013044  -0.59936666 -0.5306773   1.7313484 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "inp_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[-0.5652139  -1.058181    1.600075    0.02332006]\n",
      "  [-0.34019783 -1.2377604   1.5330346   0.04492371]\n",
      "  [ 0.36752525 -1.4228351   1.3287865  -0.27347657]\n",
      "  [ 0.09472078 -1.353683    1.455942   -0.19697987]\n",
      "  [-0.38392043 -1.0940721   1.6231282  -0.14513555]\n",
      "  [-0.41729763 -1.0276328   1.6514218  -0.20649138]\n",
      "  [-0.33023417 -1.0454823   1.6500466  -0.27433026]\n",
      "  [-0.192321   -1.1254803   1.6149355  -0.2971342 ]\n",
      "  [ 0.40822834 -1.3586452   1.3515031  -0.40108633]\n",
      "  [ 0.19979596 -1.4183375   1.3857942  -0.16725269]]\n",
      "\n",
      " [[-0.5650454  -1.0544487   1.6026778   0.01681644]\n",
      "  [-0.36043382 -1.2348608   1.5300138   0.06528069]\n",
      "  [ 0.24521795 -1.4295444   1.3651297  -0.18080315]\n",
      "  [-0.06483456 -1.3449188   1.4773034  -0.06755017]\n",
      "  [-0.4188528  -1.0775512   1.6267892  -0.13038513]\n",
      "  [-0.40018177 -1.033853    1.650498   -0.21646313]\n",
      "  [-0.35319275 -1.0375834   1.6523482  -0.26157206]\n",
      "  [-0.24463175 -1.1371143   1.6107951  -0.22904913]\n",
      "  [ 0.19615406 -1.3627281   1.4271017  -0.26052773]\n",
      "  [ 0.0841998  -1.3687493   1.4467623  -0.16221283]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "decoder_layer1_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer1_block2.shape: (2, 2, 10, 8)\n",
      "decoder_layer2_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer2_block2.shape: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Decoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 遮罩\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化一個 Decoder\n",
    "decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列以及遮罩丟入 Decoder\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_padding_mask:\", inp_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "dec_out, attn = decoder(tar, enc_out, training=False, \n",
    "                        combined_mask=combined_mask,\n",
    "                        inp_padding_mask=inp_padding_mask)\n",
    "print(\"dec_out:\", dec_out)\n",
    "print(\"-\" * 20)\n",
    "for block_name, attn_weights in attn.items():\n",
    "  print(f\"{block_name}.shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 之上已經沒有其他 layers 了，我們使用 tf.keras.Model 建立一個模型\n",
    "class Transformer(tf.keras.Model):\n",
    "  # 初始參數包含 Encoder & Decoder 都需要超參數以及中英字典數目\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, rate)\n",
    "    # 這個 FFN 輸出跟中文字典一樣大的 logits 數，等通過 softmax 就代表每個中文字的出現機率\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "  # enc_padding_mask 跟 dec_padding_mask 都是英文序列的 padding mask，\n",
    "  # 只是一個給 Encoder layer 的 MHA 用，一個是給 Decoder layer 的 MHA 2 使用\n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           combined_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "    \n",
    "    # 將 Decoder 輸出通過最後一個 linear layer\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_inp: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "tar_real: tf.Tensor(\n",
      "[[  10  241   86   27    3 4206    0    0    0]\n",
      " [ 165  489  398  191   14    7  560    3 4206]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "predictions: tf.Tensor(\n",
      "[[[ 0.01349578 -0.00199539 -0.00217387 ... -0.03862738 -0.03212878\n",
      "   -0.07692746]\n",
      "  [ 0.037483    0.01585472 -0.02548709 ... -0.04276202 -0.02495992\n",
      "   -0.05491884]\n",
      "  [ 0.05718528  0.0288353  -0.04577482 ... -0.0450176  -0.01315334\n",
      "   -0.03639908]\n",
      "  ...\n",
      "  [ 0.01202047 -0.00400385 -0.00099437 ... -0.03859971 -0.03085513\n",
      "   -0.07979751]\n",
      "  [ 0.0235797   0.00501019 -0.0119309  ... -0.04091505 -0.02892826\n",
      "   -0.0693901 ]\n",
      "  [ 0.04867784  0.02382023 -0.03683804 ... -0.04392422 -0.01941059\n",
      "   -0.04347047]]\n",
      "\n",
      " [[ 0.01676657 -0.00080312 -0.00556348 ... -0.03981712 -0.0293731\n",
      "   -0.07665333]\n",
      "  [ 0.03873825  0.0160716  -0.0268527  ... -0.04328423 -0.0234593\n",
      "   -0.05522631]\n",
      "  [ 0.05640829  0.02865588 -0.04492005 ... -0.04475704 -0.014088\n",
      "   -0.03639095]\n",
      "  ...\n",
      "  [ 0.01514172 -0.00298804 -0.00426158 ... -0.03976889 -0.02800199\n",
      "   -0.07974622]\n",
      "  [ 0.02867933  0.00800283 -0.01704068 ... -0.04215823 -0.02618419\n",
      "   -0.06638923]\n",
      "  [ 0.05056309  0.02489874 -0.03880978 ... -0.04421616 -0.01803544\n",
      "   -0.04204437]]], shape=(2, 9, 4207), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 1\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# + 2 是為了 <start> & <end> token\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "output_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 重點中的重點。訓練時用前一個字來預測下一個中文字\n",
    "tar_inp = tar[:, :-1]\n",
    "tar_real = tar[:, 1:]\n",
    "\n",
    "# 來源 / 目標語言用的遮罩。注意 `comined_mask` 已經將目標語言的兩種遮罩合而為一\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar_inp)\n",
    "look_ahead_mask = create_look_ahead_mask(tar_inp.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化我們的第一個 transformer\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, \n",
    "                          input_vocab_size, output_vocab_size)\n",
    "\n",
    "# 將英文、中文序列丟入取得 Transformer 預測下個中文字的結果\n",
    "predictions, attn_weights = transformer(inp, tar_inp, False, inp_padding_mask, \n",
    "                                        combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_inp:\", tar_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_real:\", tar_real)\n",
    "print(\"-\" * 20)\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.31326166, 0.31326166, 1.3132616 ], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# 假設我們要解的是一個 binary classifcation， 0 跟 1 個代表一個 label\n",
    "real = tf.constant([1, 1, 0], shape=(1, 3), dtype=tf.float32)\n",
    "pred = tf.constant([[0, 1], [0, 1], [0, 1]], dtype=tf.float32)\n",
    "loss_object(real, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: tf.Tensor(\n",
      "[[[ 0.01349578 -0.00199539 -0.00217387 ... -0.03862738 -0.03212878\n",
      "   -0.07692746]\n",
      "  [ 0.037483    0.01585472 -0.02548709 ... -0.04276202 -0.02495992\n",
      "   -0.05491884]\n",
      "  [ 0.05718528  0.0288353  -0.04577482 ... -0.0450176  -0.01315334\n",
      "   -0.03639908]\n",
      "  ...\n",
      "  [ 0.01202047 -0.00400385 -0.00099437 ... -0.03859971 -0.03085513\n",
      "   -0.07979751]\n",
      "  [ 0.0235797   0.00501019 -0.0119309  ... -0.04091505 -0.02892826\n",
      "   -0.0693901 ]\n",
      "  [ 0.04867784  0.02382023 -0.03683804 ... -0.04392422 -0.01941059\n",
      "   -0.04347047]]\n",
      "\n",
      " [[ 0.01676657 -0.00080312 -0.00556348 ... -0.03981712 -0.0293731\n",
      "   -0.07665333]\n",
      "  [ 0.03873825  0.0160716  -0.0268527  ... -0.04328423 -0.0234593\n",
      "   -0.05522631]\n",
      "  [ 0.05640829  0.02865588 -0.04492005 ... -0.04475704 -0.014088\n",
      "   -0.03639095]\n",
      "  ...\n",
      "  [ 0.01514172 -0.00298804 -0.00426158 ... -0.03976889 -0.02800199\n",
      "   -0.07974622]\n",
      "  [ 0.02867933  0.00800283 -0.01704068 ... -0.04215823 -0.02618419\n",
      "   -0.06638923]\n",
      "  [ 0.05056309  0.02489874 -0.03880978 ... -0.04421616 -0.01803544\n",
      "   -0.04204437]]], shape=(2, 9, 4207), dtype=float32)\n",
      "--------------------\n",
      "tf.Tensor(\n",
      "[[1.3761915 2.935208  3.8687305 3.4191077 2.6083562 1.566435  1.1489875\n",
      "  1.9882668 3.5525477]\n",
      " [1.4309781 2.9219131 3.8738992 3.500917  2.6499157 1.6611676 1.1839242\n",
      "  2.2150595 3.6206636]], shape=(2, 9), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"predictions:\", predictions)\n",
    "print(\"-\" * 20)\n",
    "print(tf.reduce_sum(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  # 這次的 mask 將序列中不等於 0 的位置視為 1，其餘為 0 \n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  # 照樣計算所有位置的 cross entropy 但不加總\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask  # 只計算非 <pad> 位置的損失 \n",
    "  \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 8115\n",
      "target_vocab_size: 4207\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "dropout_rate = 0.1  # 預設值\n",
    "\n",
    "print(\"input_vocab_size:\", input_vocab_size)\n",
    "print(\"target_vocab_size:\", target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  # 論文預設 `warmup_steps` = 4000\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "# 將客製化 learning rate schdeule 丟入 Adam opt.\n",
    "# Adam opt. 的參數都跟論文相同\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_models = [128, 256, 512]\n",
    "warmup_steps = [1000 * i for i in range(1, 4)]\n",
    "\n",
    "schedules = []\n",
    "labels = []\n",
    "colors = [\"blue\", \"red\", \"black\"]\n",
    "for d in d_models:\n",
    "  schedules += [CustomSchedule(d, s) for s in warmup_steps]\n",
    "  labels += [f\"d_model: {d}, warm: {s}\" for s in warmup_steps]\n",
    "\n",
    "for i, (schedule, label) in enumerate(zip(schedules, labels)):\n",
    "  plt.plot(schedule(tf.range(10000, dtype=tf.float32)), \n",
    "           label=label, color=colors[i // 3])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這個 Transformer 有 4 層 Encoder / Decoder layers\n",
      "d_model: 128\n",
      "num_heads: 8\n",
      "dff: 512\n",
      "input_vocab_size: 8115\n",
      "target_vocab_size: 4207\n",
      "dropout_rate: 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "print(f\"\"\"這個 Transformer 有 {num_layers} 層 Encoder / Decoder layers\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "input_vocab_size: {input_vocab_size}\n",
    "target_vocab_size: {target_vocab_size}\n",
    "dropout_rate: {dropout_rate}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒找到 checkpoint，從頭訓練。\n"
     ]
    }
   ],
   "source": [
    "# 方便比較不同實驗/ 不同超參數設定的結果\n",
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_perc}train_perc\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)\n",
    "\n",
    "# tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取\n",
    "# 一般來說你會想存下模型以及 optimizer 的狀態\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西\n",
    "# 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果在 checkpoint 路徑上有發現檔案就讀進來\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  \n",
    "  # 用來確認之前訓練多少 epochs 了\n",
    "  last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "  print(f'已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。')\n",
    "else:\n",
    "  last_epoch = 0\n",
    "  print(\"沒找到 checkpoint，從頭訓練。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為 Transformer 的 Encoder / Decoder 準備遮罩\n",
    "def create_masks(inp, tar):\n",
    "  # 英文句子的 padding mask，要交給 Encoder layer 自注意力機制用的\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # 同樣也是英文句子的 padding mask，但是是要交給 Decoder layer 的 MHA 2 \n",
    "  # 關注 Encoder 輸出序列用的\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Decoder layer 的 MHA1 在做自注意力機制用的\n",
    "  # `combined_mask` 是中文句子的 padding mask 跟 look ahead mask 的疊加\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function  # 讓 TensorFlow 幫我們將 eager code 優化並加快運算\n",
    "def train_step(inp, tar):\n",
    "  # 前面說過的，用去尾的原始序列去預測下一個字的序列\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  # 建立 3 個遮罩\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  # 紀錄 Transformer 的所有運算過程以方便之後做梯度下降\n",
    "  with tf.GradientTape() as tape:\n",
    "    # 注意是丟入 `tar_inp` 而非 `tar`。記得將 `training` 參數設定為 True\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    # 跟影片中顯示的相同，計算左移一個字的序列跟模型預測分佈之間的差異，當作 loss\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  # 取出梯度並呼叫前面定義的 Adam optimizer 幫我們更新 Transformer 裡頭可訓練的參數\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  # 將 loss 以及訓練 acc 記錄到 TensorBoard 上，非必要\n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此超參數組合的 Transformer 已經訓練 0 epochs。\n",
      "剩餘 epochs：-30\n",
      "Saving checkpoint for epoch 1 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-1\n",
      "Epoch 1 Loss 5.1833 Accuracy 0.0223\n",
      "Time taken for 1 epoch: 386.8310549259186 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-2\n",
      "Epoch 2 Loss 4.2405 Accuracy 0.0606\n",
      "Time taken for 1 epoch: 303.06020998954773 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-3\n",
      "Epoch 3 Loss 3.7393 Accuracy 0.0990\n",
      "Time taken for 1 epoch: 298.4722592830658 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-4\n",
      "Epoch 4 Loss 3.2641 Accuracy 0.1511\n",
      "Time taken for 1 epoch: 292.25172114372253 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-5\n",
      "Epoch 5 Loss 2.9636 Accuracy 0.1810\n",
      "Time taken for 1 epoch: 298.7796070575714 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-6\n",
      "Epoch 6 Loss 2.7774 Accuracy 0.1980\n",
      "Time taken for 1 epoch: 293.79499220848083 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-7\n",
      "Epoch 7 Loss 2.6359 Accuracy 0.2117\n",
      "Time taken for 1 epoch: 279.6301519870758 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-8\n",
      "Epoch 8 Loss 2.5204 Accuracy 0.2241\n",
      "Time taken for 1 epoch: 296.7212281227112 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-9\n",
      "Epoch 9 Loss 2.4197 Accuracy 0.2350\n",
      "Time taken for 1 epoch: 293.95908784866333 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-10\n",
      "Epoch 10 Loss 2.3199 Accuracy 0.2462\n",
      "Time taken for 1 epoch: 329.8636758327484 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-11\n",
      "Epoch 11 Loss 2.2191 Accuracy 0.2581\n",
      "Time taken for 1 epoch: 283.4793269634247 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-12\n",
      "Epoch 12 Loss 2.1257 Accuracy 0.2700\n",
      "Time taken for 1 epoch: 282.3798499107361 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-13\n",
      "Epoch 13 Loss 2.0359 Accuracy 0.2818\n",
      "Time taken for 1 epoch: 279.08454871177673 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-14\n",
      "Epoch 14 Loss 1.9516 Accuracy 0.2933\n",
      "Time taken for 1 epoch: 286.8188409805298 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-15\n",
      "Epoch 15 Loss 1.8764 Accuracy 0.3042\n",
      "Time taken for 1 epoch: 290.7323491573334 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-16\n",
      "Epoch 16 Loss 1.8065 Accuracy 0.3138\n",
      "Time taken for 1 epoch: 284.24292612075806 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-17\n",
      "Epoch 17 Loss 1.7430 Accuracy 0.3233\n",
      "Time taken for 1 epoch: 285.6288478374481 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-18\n",
      "Epoch 18 Loss 1.6765 Accuracy 0.3326\n",
      "Time taken for 1 epoch: 283.58339524269104 secs\n",
      "\n",
      "Saving checkpoint for epoch 19 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-19\n",
      "Epoch 19 Loss 1.6102 Accuracy 0.3424\n",
      "Time taken for 1 epoch: 278.8892939090729 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-20\n",
      "Epoch 20 Loss 1.5464 Accuracy 0.3519\n",
      "Time taken for 1 epoch: 281.89067006111145 secs\n",
      "\n",
      "Saving checkpoint for epoch 21 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-21\n",
      "Epoch 21 Loss 1.4882 Accuracy 0.3612\n",
      "Time taken for 1 epoch: 278.58326482772827 secs\n",
      "\n",
      "Saving checkpoint for epoch 22 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-22\n",
      "Epoch 22 Loss 1.4361 Accuracy 0.3686\n",
      "Time taken for 1 epoch: 277.23466897010803 secs\n",
      "\n",
      "Saving checkpoint for epoch 23 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-23\n",
      "Epoch 23 Loss 1.3904 Accuracy 0.3751\n",
      "Time taken for 1 epoch: 277.32534289360046 secs\n",
      "\n",
      "Saving checkpoint for epoch 24 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-24\n",
      "Epoch 24 Loss 1.3463 Accuracy 0.3817\n",
      "Time taken for 1 epoch: 278.5759689807892 secs\n",
      "\n",
      "Saving checkpoint for epoch 25 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-25\n",
      "Epoch 25 Loss 1.3075 Accuracy 0.3875\n",
      "Time taken for 1 epoch: 279.9417338371277 secs\n",
      "\n",
      "Saving checkpoint for epoch 26 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-26\n",
      "Epoch 26 Loss 1.2707 Accuracy 0.3929\n",
      "Time taken for 1 epoch: 281.8735728263855 secs\n",
      "\n",
      "Saving checkpoint for epoch 27 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-27\n",
      "Epoch 27 Loss 1.2378 Accuracy 0.3979\n",
      "Time taken for 1 epoch: 280.72323203086853 secs\n",
      "\n",
      "Saving checkpoint for epoch 28 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-28\n",
      "Epoch 28 Loss 1.2069 Accuracy 0.4027\n",
      "Time taken for 1 epoch: 280.2357859611511 secs\n",
      "\n",
      "Saving checkpoint for epoch 29 at nmt/checkpoints/4layers_128d_8heads_512dff_20train_perc/ckpt-29\n",
      "Epoch 29 Loss 1.1778 Accuracy 0.4069\n",
      "Time taken for 1 epoch: 309.66522908210754 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定義我們要看幾遍數據集\n",
    "EPOCHS = 30\n",
    "print(f\"此超參數組合的 Transformer 已經訓練 {last_epoch} epochs。\")\n",
    "print(f\"剩餘 epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "\n",
    "\n",
    "# 用來寫資訊到 TensorBoard，非必要但十分推薦\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# 比對設定的 `EPOCHS` 以及已訓練的 `last_epoch` 來決定還要訓練多少 epochs\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  # 重置紀錄 TensorBoard 的 metrics\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # 一個 epoch 就是把我們定義的訓練資料集一個一個 batch 拿出來處理，直到看完整個數據集 \n",
    "  for (step_idx, (inp, tar)) in enumerate(train_dataset):\n",
    "    \n",
    "    # 每次 step 就是將數據丟入 Transformer，讓它生預測結果並計算梯度最小化 loss\n",
    "    train_step(inp, tar)  \n",
    "\n",
    "  # 每個 epoch 完成就存一次檔    \n",
    "  if (epoch + 1) % 1 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  # 將 loss 以及 accuracy 寫到 TensorBoard 上\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch + 1)\n",
    "    tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch + 1)\n",
    "  \n",
    "  print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "  print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict\n",
    "def evaluate(inp_sentence):\n",
    "  \n",
    "  # 準備英文句子前後會加上的 <start>, <end>\n",
    "  start_token = [subword_encoder_en.vocab_size]\n",
    "  end_token = [subword_encoder_en.vocab_size + 1]\n",
    "  \n",
    "  # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列\n",
    "  # 並在前後加上 BOS / EOS\n",
    "  inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入\n",
    "  # 是一個只包含一個中文 <start> token 的序列\n",
    "  decoder_input = [subword_encoder_zh.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)  # 增加 batch 維度\n",
    "  \n",
    "  # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 每多一個生成的字就得產生新的遮罩\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "\n",
    "    # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字\n",
    "    predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # 遇到 <end> token 就停止回傳，代表模型已經產生完結果\n",
    "    if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生\n",
    "    # 下個中文字的時候關注到最新的 `predicted_id`\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  # 將 batch 的維度去掉後回傳預測的中文索引序列\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opencc import OpenCC\n",
    "cc = OpenCC('s2t')\n",
    "# 要被翻譯的英文句子\n",
    "sentence = \"China, India, and others have enjoyed continuing economic growth.\"\n",
    "\n",
    "# 取得預測的中文索引序列\n",
    "predicted_seq, _ = evaluate(sentence)\n",
    "\n",
    "# 過濾掉 <start> & <end> tokens 並用中文的 subword tokenizer 幫我們將索引序列還原回中文句子\n",
    "target_vocab_size = subword_encoder_zh.vocab_size\n",
    "predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "predicted_sentence = subword_encoder_zh.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "print(\"sentence:\", sentence)\n",
    "print(\"predicted_sentence:\", cc.convert(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = open('../translation2019zh/translation2019zh_valid.json', 'r', encoding='utf-8') \n",
    "datas = []\n",
    "for line in file.readlines():\n",
    "    dic = json.loads(line)\n",
    "    datas.append(dic)\n",
    "    \n",
    "for index,data in enumerate(datas):\n",
    "    if index<50 :\n",
    "        # 要被翻譯的英文句子\n",
    "        sentence = data['english']\n",
    "\n",
    "        # 取得預測的中文索引序列\n",
    "        predicted_seq, _ = evaluate(sentence)\n",
    "\n",
    "        # 過濾掉 <start> & <end> tokens 並用中文的 subword tokenizer 幫我們將索引序列還原回中文句子\n",
    "        target_vocab_size = subword_encoder_zh.vocab_size\n",
    "        predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "        predicted_sentence = subword_encoder_zh.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "        print(\"sentence:\", sentence)\n",
    "        print(\"predicted_sentence:\", cc.convert(predicted_sentence))\n",
    "        print(\"-\"*30)\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
