{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=\"ERROR\")\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path\n",
    "output_dir = \"nmt\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Split('train'): ['newscommentary_v14',\n",
      "                  'wikititles_v1',\n",
      "                  'uncorpus_v1',\n",
      "                  'casia2015',\n",
      "                  'casict2011',\n",
      "                  'casict2015',\n",
      "                  'datum2015',\n",
      "                  'datum2017',\n",
      "                  'neu2017'],\n",
      " Split('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(tmp_builder.subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在設定檔 config 裡頭指定新聞評論這個資料來源並請 TensorFlow Datasets 下載\n",
    "config = tfds.translate.wmt.WmtConfig(\n",
    "  version=tfds.core.Version('0.0.3'),\n",
    "  language_pair=(\"zh\", \"en\"),\n",
    "  subsets={\n",
    "    tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "  }\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n",
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "examples = builder.as_dataset(split=['train[:20%]','train[20%:21%]','train[21%:]'], as_supervised=True)\n",
    "\n",
    "train_examples, val_examples, _ = examples\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'The fear is real and visceral, and politicians ignore it at their peril.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word \\xe2\\x80\\x9cliberal\\xe2\\x80\\x9d \\xe2\\x80\\x93 a champion of the cause of individual freedom.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe4\\xba\\x8b\\xe5\\xae\\x9e\\xe4\\xb8\\x8a\\xef\\xbc\\x8c\\xe5\\xbe\\xb7\\xe5\\x9b\\xbd\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xb1\\x80\\xe5\\x8a\\xbf\\xe9\\x9c\\x80\\xe8\\xa6\\x81\\xe7\\x9a\\x84\\xe4\\xb8\\x8d\\xe8\\xbf\\x87\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe7\\xac\\xa6\\xe5\\x90\\x88\\xe7\\xbe\\x8e\\xe5\\x9b\\xbd\\xe6\\x89\\x80\\xe8\\xb0\\x93\\xe2\\x80\\x9c\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe2\\x80\\x9d\\xe5\\xae\\x9a\\xe4\\xb9\\x89\\xe7\\x9a\\x84\\xe7\\x9c\\x9f\\xe6\\xad\\xa3\\xe7\\x9a\\x84\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe5\\x85\\x9a\\xe6\\xb4\\xbe\\xef\\xbc\\x8c\\xe4\\xb9\\x9f\\xe5\\xb0\\xb1\\xe6\\x98\\xaf\\xe4\\xb8\\xaa\\xe4\\xba\\xba\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe4\\xba\\x8b\\xe4\\xb8\\x9a\\xe7\\x9a\\x84\\xe5\\x80\\xa1\\xe5\\xaf\\xbc\\xe8\\x80\\x85\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xbf\\x85\\xe9\\xa1\\xbb\\xe4\\xbb\\x98\\xe5\\x87\\xba\\xe5\\xb7\\xa8\\xe5\\xa4\\xa7\\xe7\\x9a\\x84\\xe5\\x8a\\xaa\\xe5\\x8a\\x9b\\xe5\\x92\\x8c\\xe5\\x9f\\xba\\xe7\\xa1\\x80\\xe8\\xae\\xbe\\xe6\\x96\\xbd\\xe6\\x8a\\x95\\xe8\\xb5\\x84\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\xae\\x8c\\xe6\\x88\\x90\\xe5\\x90\\x91\\xe5\\x8f\\xaf\\xe5\\x86\\x8d\\xe7\\x94\\x9f\\xe8\\x83\\xbd\\xe6\\xba\\x90\\xe7\\x9a\\x84\\xe8\\xbf\\x87\\xe6\\xb8\\xa1\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for en, zh in train_examples.take(3):\n",
    "  print(en)\n",
    "  print(zh)\n",
    "  print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fear is real and visceral, and politicians ignore it at their peril.\n",
      "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
      "----------\n",
      "In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word “liberal” – a champion of the cause of individual freedom.\n",
      "事实上，德国政治局势需要的不过是一个符合美国所谓“自由”定义的真正的自由党派，也就是个人自由事业的倡导者。\n",
      "----------\n",
      "Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.\n",
      "必须付出巨大的努力和基础设施投资才能完成向可再生能源的过渡。\n",
      "----------\n",
      "In this sense, it is critical to recognize the fundamental difference between “urban villages” and their rural counterparts.\n",
      "在这方面，关键在于认识到“城市村落”和农村村落之间的根本区别。\n",
      "----------\n",
      "A strong European voice, such as Nicolas Sarkozy’s during the French presidency of the EU, may make a difference, but only for six months, and at the cost of reinforcing other European countries’ nationalist feelings in reaction to the expression of “Gallic pride.”\n",
      "法国担任轮值主席国期间尼古拉·萨科奇统一的欧洲声音可能让人耳目一新，但这种声音却只持续了短短六个月，而且付出了让其他欧洲国家在面对“高卢人的骄傲”时民族主义情感进一步被激发的代价。\n",
      "----------\n",
      "Most of Japan’s bondholders are nationals (if not the central bank) and have an interest in political stability.\n",
      "日本债券持有人大多为本国国民（甚至中央银行 ） ， 政治稳定符合他们的利益。\n",
      "----------\n",
      "Paul Romer, one of the originators of new growth theory, has accused some leading names, including the Nobel laureate Robert Lucas, of what he calls “mathiness” – using math to obfuscate rather than clarify.\n",
      "新增长理论创始人之一的保罗·罗默（Paul Romer）也批评一些著名经济学家，包括诺贝尔奖获得者罗伯特·卢卡斯（Robert Lucas）在内，说他们“数学性 ” （ 罗默的用语）太重，结果是让问题变得更加模糊而不是更加清晰。\n",
      "----------\n",
      "It is, in fact, a capsule depiction of the United States Federal Reserve and the European Central Bank.\n",
      "事实上，这就是对美联储和欧洲央行的简略描述。\n",
      "----------\n",
      "Given these variables, the degree to which migration is affected by asylum-seekers will not be easy to predict or control.\n",
      "考虑到这些变量，移民受寻求庇护者的影响程度很难预测或控制。\n",
      "----------\n",
      "WASHINGTON, DC – In the 2016 American presidential election, Hillary Clinton and Donald Trump agreed that the US economy is suffering from dilapidated infrastructure, and both called for greater investment in renovating and upgrading the country’s public capital stock.\n",
      "华盛顿—在2016年美国总统选举中，希拉里·克林顿和唐纳德·特朗普都认为美国经济饱受基础设施陈旧的拖累，两人都要求加大投资用于修缮和升级美国公共资本存量。\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "  en = en_t.numpy().decode(\"utf-8\")\n",
    "  zh = zh_t.numpy().decode(\"utf-8\")\n",
    "  \n",
    "  print(en)\n",
    "  print(zh)\n",
    "  print('-' * 10)\n",
    "  \n",
    "  # 之後用來簡單評估模型的訓練情況\n",
    "  sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：8113\n",
      "前 10 個 subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'is_', 'that_']\n",
      "\n",
      "CPU times: user 1min 27s, sys: 5.92 s, total: 1min 33s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#建立中文與英文字典\n",
    "try:\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "    print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (en.numpy() for en, _ in train_examples), \n",
    "      target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    "  \n",
    "    # 將字典檔案存下以方便下次 warmstart\n",
    "    subword_encoder_en.save_to_file(en_vocab_file)\n",
    "  \n",
    "\n",
    "print(f\"字典大小：{subword_encoder_en.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_en.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125, 5202, 7889, 523, 6062, 7903]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = 'I love my family.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Subword\n",
      "---------------\n",
      "  125     I \n",
      " 5202     love\n",
      " 7889      \n",
      "  523     my \n",
      " 6062     family\n",
      " 7903     .\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:10}{1:6}\".format(\"Index\", \"Subword\"))\n",
    "print(\"-\" * 15)\n",
    "for idx in indices:\n",
    "    subword = subword_encoder_en.decode([idx])\n",
    "    print('{0:5}{1:6}'.format(idx, ' ' * 5 + subword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I love my family.', 'I love my family.')\n"
     ]
    }
   ],
   "source": [
    "indices = subword_encoder_en.encode(sample_string)\n",
    "decoded_string = subword_encoder_en.decode(indices)\n",
    "assert decoded_string == sample_string\n",
    "pprint((sample_string, decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：4205\n",
      "前 10 個 subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\n",
      "\n",
      "CPU times: user 7min 23s, sys: 9.01 s, total: 7min 32s\n",
      "Wall time: 8min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#建立中文字典\n",
    "try:\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "    print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (zh.numpy() for _, zh in train_examples), \n",
    "      target_vocab_size=2**13, # 有需要可以調整字典大小\n",
    "      max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "\n",
    "    # 將字典檔案存下以方便下次 warmstart \n",
    "    subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f\"字典大小：{subword_encoder_zh.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[英中原文]（轉換前）\n",
      "The eurozone’s collapse forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "\n",
      "--------------------\n",
      "\n",
      "[英中序列]（轉換後）\n",
      "[16, 900, 11, 6, 1527, 874, 8, 230, 2259, 2728, 239, 3, 89, 1236, 7903]\n",
      "[44, 202, 168, 1, 852, 201, 231, 592, 44, 87, 17, 124, 106, 38, 7, 279, 86, 18, 212, 265, 3]\n"
     ]
    }
   ],
   "source": [
    "en = \"The eurozone’s collapse forces a major realignment of European politics.\"\n",
    "zh = \"欧元区的瓦解强迫欧洲政治进行一次重大改组。\"\n",
    "\n",
    "# 將文字轉成為 subword indices\n",
    "en_indices = subword_encoder_en.encode(en)\n",
    "zh_indices = subword_encoder_zh.encode(zh)\n",
    "\n",
    "print(\"[英中原文]（轉換前）\")\n",
    "print(en)\n",
    "print(zh)\n",
    "print()\n",
    "print('-' * 20)\n",
    "print()\n",
    "print(\"[英中序列]（轉換後）\")\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_t, zh_t):\n",
    "  # 因為字典的索引從 0 開始，\n",
    "  # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "  # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "  en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "  # 同理，不過是使用中文字典的最後一個索引 + 1\n",
    "  zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "  return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 BOS 的 index： 8113\n",
      "英文 EOS 的 index： 8114\n",
      "中文 BOS 的 index： 4205\n",
      "中文 EOS 的 index： 4206\n",
      "\n",
      "輸入為 2 個 Tensors：\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'The fear is real and visceral, and politicians ignore it at their peril.'>,\n",
      " <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82'>)\n",
      "---------------\n",
      "輸出為 2 個索引序列：\n",
      "([8113,\n",
      "  16,\n",
      "  1284,\n",
      "  9,\n",
      "  243,\n",
      "  5,\n",
      "  1275,\n",
      "  1756,\n",
      "  156,\n",
      "  1,\n",
      "  5,\n",
      "  1016,\n",
      "  5566,\n",
      "  21,\n",
      "  38,\n",
      "  33,\n",
      "  2982,\n",
      "  7965,\n",
      "  7903,\n",
      "  8114],\n",
      " [4205,\n",
      "  10,\n",
      "  151,\n",
      "  574,\n",
      "  1298,\n",
      "  6,\n",
      "  374,\n",
      "  55,\n",
      "  29,\n",
      "  193,\n",
      "  5,\n",
      "  1,\n",
      "  3,\n",
      "  3981,\n",
      "  931,\n",
      "  431,\n",
      "  125,\n",
      "  1,\n",
      "  17,\n",
      "  124,\n",
      "  33,\n",
      "  20,\n",
      "  97,\n",
      "  1089,\n",
      "  1247,\n",
      "  861,\n",
      "  3,\n",
      "  4206])\n"
     ]
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "print('英文 BOS 的 index：', subword_encoder_en.vocab_size)\n",
    "print('英文 EOS 的 index：', subword_encoder_en.vocab_size + 1)\n",
    "print('中文 BOS 的 index：', subword_encoder_zh.vocab_size)\n",
    "print('中文 EOS 的 index：', subword_encoder_zh.vocab_size + 1)\n",
    "\n",
    "print('\\n輸入為 2 個 Tensors：')\n",
    "pprint((en_t, zh_t))\n",
    "print('-' * 15)\n",
    "print('輸出為 2 個索引序列：')\n",
    "pprint((en_indices, zh_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[8113   16 1284    9  243    5 1275 1756  156    1    5 1016 5566   21\n",
      "   38   33 2982 7965 7903 8114], shape=(20,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[4205   10  151  574 1298    6  374   55   29  193    5    1    3 3981\n",
      "  931  431  125    1   17  124   33   20   97 1089 1247  861    3 4206], shape=(28,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def tf_encode(en_t, zh_t):\n",
    "  # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors\n",
    "  # 要到 `tf.py_funtion` 裡頭才是\n",
    "  # 另外因為索引都是整數，所以使用 `tf.int64`\n",
    "  return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "# `tmp_dataset` 為說明用資料集，說明完所有重要的 func，\n",
    "# 我們會從頭建立一個正式的 `train_dataset`\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "  # en, zh 分別代表英文與中文的索引序列\n",
    "  return tf.logical_and(tf.size(en) <= max_length,\n",
    "                        tf.size(zh) <= max_length)\n",
    "\n",
    "# tf.data.Dataset.filter(func) 只會回傳 func 為真的例子\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有英文與中文序列長度都不超過 40 個 tokens\n",
      "訓練資料集裡總共有 29784 筆數據\n"
     ]
    }
   ],
   "source": [
    "# 因為我們數據量小可以這樣 count\n",
    "num_examples = 0\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "  cond1 = len(en_indices) <= MAX_LENGTH\n",
    "  cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "  assert cond1 and cond2\n",
    "  num_examples += 1\n",
    "\n",
    "print(f\"所有英文與中文序列長度都不超過 {MAX_LENGTH} 個 tokens\")\n",
    "print(f\"訓練資料集裡總共有 {num_examples} 筆數據\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113   16 1284 ...    0    0    0]\n",
      " [8113 1894 1302 ...    0    0    0]\n",
      " [8113   44   40 ...    0    0    0]\n",
      " ...\n",
      " [8113  122  506 ...    0    0    0]\n",
      " [8113   16  215 ...    0    0    0]\n",
      " [8113 7443 7889 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205   10  151 ...    0    0    0]\n",
      " [4205  206  275 ...    0    0    0]\n",
      " [4205    5   10 ...    0    0    0]\n",
      " ...\n",
      " [4205   34    6 ...    0    0    0]\n",
      " [4205  317  256 ...    0    0    0]\n",
      " [4205  167  326 ...    0    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 將 batch 裡的所有序列都 pad 到同樣長度\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "# 訓練集\n",
    "train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                 .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                 .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                 .cache() # 加快讀取數據\n",
    "                 .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                 .padded_batch(BATCH_SIZE, # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "# 驗證集\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113 3093    1 ...    0    0    0]\n",
      " [8113 2478 2396 ...    0    0    0]\n",
      " [8113 1063 1066 ...    0    0    0]\n",
      " ...\n",
      " [8113  122   12 ...    0    0    0]\n",
      " [8113 3563  775 ...    0    0    0]\n",
      " [8113   41    2 ...    0    0    0]], shape=(128, 37), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205    9  717 ...    0    0    0]\n",
      " [4205    4   33 ...    0    0    0]\n",
      " [4205   71   86 ...    0    0    0]\n",
      " ...\n",
      " [4205   29   62 ...    0    0    0]\n",
      " [4205  104  112 ...    0    0    0]\n",
      " [4205   34    6 ...    3 4206    0]], shape=(128, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It is important.', '这很重要。'),\n",
      " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "pprint(demo_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print('' * 10)\n",
    "print('tar:', tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       " array([[[-0.00560867,  0.04747799, -0.02269538,  0.02303809],\n",
       "         [ 0.02988834,  0.00620367,  0.03613586, -0.02179264],\n",
       "         [ 0.01707463,  0.03969477,  0.02194964, -0.03702695],\n",
       "         [ 0.01390957,  0.0195005 ,  0.03259501,  0.02743271],\n",
       "         [ 0.02233395, -0.04510158, -0.00677161,  0.04184886],\n",
       "         [-0.00289515, -0.02603846, -0.04859028,  0.00019344],\n",
       "         [-0.02180409, -0.00259377,  0.01643033, -0.01761655],\n",
       "         [-0.02180409, -0.00259377,  0.01643033, -0.01761655]],\n",
       " \n",
       "        [[-0.00560867,  0.04747799, -0.02269538,  0.02303809],\n",
       "         [ 0.00088333,  0.02109436,  0.0129119 , -0.03082365],\n",
       "         [ 0.01852404,  0.04630077, -0.02833566, -0.02577083],\n",
       "         [ 0.04514705,  0.03137792, -0.04057758,  0.04044752],\n",
       "         [ 0.04274544,  0.02157653, -0.04907006,  0.02949495],\n",
       "         [-0.02063758, -0.04873876, -0.03854288,  0.020351  ],\n",
       "         [ 0.02233395, -0.04510158, -0.00677161,  0.04184886],\n",
       "         [-0.00289515, -0.02603846, -0.04859028,  0.00019344]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       " array([[[ 0.04993161,  0.02566061, -0.0146607 ,  0.00574071],\n",
       "         [ 0.01592635, -0.02025861,  0.00020757, -0.04203112],\n",
       "         [ 0.03664401,  0.00267266,  0.01699981, -0.04485391],\n",
       "         [ 0.02957764,  0.02617563, -0.01678916,  0.00045334],\n",
       "         [-0.03879989, -0.01194926,  0.03493711, -0.04563121],\n",
       "         [-0.01690708,  0.03204078,  0.04683626,  0.02910907],\n",
       "         [-0.02911331,  0.02794905,  0.01279471, -0.03680716],\n",
       "         [ 0.04868478, -0.0262701 ,  0.02982837,  0.04889109],\n",
       "         [ 0.04868478, -0.0262701 ,  0.02982837,  0.04889109],\n",
       "         [ 0.04868478, -0.0262701 ,  0.02982837,  0.04889109]],\n",
       " \n",
       "        [[ 0.04993161,  0.02566061, -0.0146607 ,  0.00574071],\n",
       "         [ 0.00005664, -0.03074064,  0.04639968,  0.0047685 ],\n",
       "         [ 0.03511107,  0.04525493, -0.03638245, -0.01101079],\n",
       "         [-0.04905901,  0.03428625,  0.00740141, -0.01450055],\n",
       "         [ 0.02496466,  0.03609515,  0.0253484 , -0.03687034],\n",
       "         [ 0.01206124, -0.02820102,  0.03362811, -0.04972026],\n",
       "         [ 0.00390694,  0.03242769,  0.02957014, -0.0285221 ],\n",
       "         [-0.00034374,  0.00425357,  0.00814558, -0.02010503],\n",
       "         [-0.01690708,  0.03204078,  0.04683626,  0.02910907],\n",
       "         [-0.02911331,  0.02794905,  0.01279471, -0.03680716]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar[0]: tf.Tensor([0 0 0], shape=(3,), dtype=int64)\n",
      "--------------------\n",
      "emb_tar[0]: tf.Tensor(\n",
      "[[ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      " [ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      " [ 0.04868478 -0.0262701   0.02982837  0.04889109]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"tar[0]:\", tar[0][-3:])\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_tar[0]:\", emb_tar[0][-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遮罩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "tf.squeeze(inp_mask): tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tf.squeeze(inp_mask):\", tf.squeeze(inp_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\n",
    "tf.random.set_seed(9527)\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "  # 將 `q`、 `k` 做點積再 scale\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "  # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # 以注意權重對 v 做加權平均（weighted average）\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[0.37517118 0.37485787 0.3749141  0.49988133]\n",
      "  [0.3748806  0.37499705 0.3749746  0.50012225]\n",
      "  [0.37480643 0.3749749  0.3748127  0.5001838 ]\n",
      "  [0.37515733 0.37485746 0.37509224 0.49988168]\n",
      "  [0.37525243 0.3749971  0.37524804 0.49981332]\n",
      "  [0.37502667 0.3750993  0.37499174 0.49998993]\n",
      "  [0.37485623 0.37504378 0.3749714  0.50007737]\n",
      "  [0.37485623 0.37504378 0.3749714  0.50007737]]\n",
      "\n",
      " [[0.6251291  0.24996676 0.6249881  0.37510455]\n",
      "  [0.6251714  0.24987376 0.6250084  0.3749109 ]\n",
      "  [0.6251849  0.24989976 0.6249447  0.37497264]\n",
      "  [0.62492216 0.25006002 0.62483686 0.37510434]\n",
      "  [0.6248967  0.2500843  0.62485194 0.37509596]\n",
      "  [0.6247307  0.25023276 0.6250605  0.3751313 ]\n",
      "  [0.6246762  0.2501855  0.62492657 0.375069  ]\n",
      "  [0.6248294  0.25016233 0.6250129  0.375083  ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "attention_weights: tf.Tensor(\n",
      "[[[0.12520449 0.12492143 0.12502347 0.12504236 0.12492433 0.12498906\n",
      "   0.12494739 0.12494739]\n",
      "  [0.12490316 0.12514745 0.12512518 0.12504764 0.12492979 0.1248524\n",
      "   0.12499719 0.12499719]\n",
      "  [0.12500356 0.12512356 0.12520888 0.12502067 0.1247822  0.12484152\n",
      "   0.12500982 0.12500982]\n",
      "  [0.12502603 0.12504958 0.12502423 0.12512913 0.12500218 0.12484696\n",
      "   0.12496093 0.12496093]\n",
      "  [0.1249385  0.12496223 0.12481622 0.1250327  0.12528123 0.12510075\n",
      "   0.12493414 0.12493414]\n",
      "  [0.12501831 0.12489989 0.12489061 0.1248925  0.12511584 0.125216\n",
      "   0.12498342 0.12498342]\n",
      "  [0.12495127 0.12501934 0.12503359 0.12498114 0.12492385 0.12495804\n",
      "   0.1250664  0.1250664 ]\n",
      "  [0.12495127 0.12501934 0.12503359 0.12498114 0.12492385 0.12495804\n",
      "   0.1250664  0.1250664 ]]\n",
      "\n",
      " [[0.12513778 0.12492909 0.1250635  0.12512262 0.12509063 0.12487613\n",
      "   0.12485777 0.12492246]\n",
      "  [0.12503712 0.12513526 0.12512644 0.12497072 0.12497191 0.12490185\n",
      "   0.12489326 0.12496345]\n",
      "  [0.12507287 0.1250277  0.12518612 0.12508868 0.12509021 0.12480955\n",
      "   0.12477899 0.12494591]\n",
      "  [0.1250443  0.1247846  0.125001   0.12524554 0.12521334 0.12484669\n",
      "   0.12494876 0.12491573]\n",
      "  [0.12501165 0.12478511 0.12500185 0.12521265 0.12519886 0.12488545\n",
      "   0.12494731 0.12495708]\n",
      "  [0.12489854 0.12481634 0.12482258 0.1249473  0.12498678 0.12524597\n",
      "   0.1251301  0.12515235]\n",
      "  [0.12488351 0.12481108 0.12479535 0.12505281 0.12505202 0.12513345\n",
      "   0.1252261  0.1250457 ]\n",
      "  [0.12492067 0.1248537  0.12493474 0.12499218 0.12503423 0.1251281\n",
      "   0.12501812 0.12511821]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"output:\", output)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "inp_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "inp_mask = create_padding_mask(inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_mask:\", inp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[0.16691592 0.16653857 0.16667458 0.1666998  0.16654243 0.16662873\n",
      "   0.         0.        ]\n",
      "  [0.16653629 0.166862   0.16683231 0.16672891 0.1665718  0.1664686\n",
      "   0.         0.        ]\n",
      "  [0.16667578 0.16683577 0.16694954 0.16669858 0.16638063 0.16645971\n",
      "   0.         0.        ]\n",
      "  [0.166684   0.1667154  0.1666816  0.16682146 0.1666522  0.16644527\n",
      "   0.         0.        ]\n",
      "  [0.16655542 0.16658705 0.16639242 0.16668099 0.16701232 0.16677172\n",
      "   0.         0.        ]\n",
      "  [0.16668372 0.16652583 0.16651346 0.16651598 0.16681376 0.1669473\n",
      "   0.         0.        ]\n",
      "  [0.1666312  0.16672198 0.16674098 0.16667102 0.16659464 0.16664024\n",
      "   0.         0.        ]\n",
      "  [0.1666312  0.16672198 0.16674098 0.16667102 0.16659464 0.16664024\n",
      "   0.         0.        ]]\n",
      "\n",
      " [[0.12513778 0.12492909 0.1250635  0.12512262 0.12509063 0.12487613\n",
      "   0.12485777 0.12492246]\n",
      "  [0.12503712 0.12513526 0.12512644 0.12497072 0.12497191 0.12490185\n",
      "   0.12489326 0.12496345]\n",
      "  [0.12507287 0.1250277  0.12518612 0.12508868 0.12509021 0.12480955\n",
      "   0.12477899 0.12494591]\n",
      "  [0.1250443  0.1247846  0.125001   0.12524554 0.12521334 0.12484669\n",
      "   0.12494876 0.12491573]\n",
      "  [0.12501165 0.12478511 0.12500185 0.12521265 0.12519886 0.12488545\n",
      "   0.12494731 0.12495708]\n",
      "  [0.12489854 0.12481634 0.12482258 0.1249473  0.12498678 0.12524597\n",
      "   0.1251301  0.12515235]\n",
      "  [0.12488351 0.12481108 0.12479535 0.12505281 0.12505202 0.12513345\n",
      "   0.1252261  0.1250457 ]\n",
      "  [0.12492067 0.1248537  0.12493474 0.12499218 0.12503423 0.1251281\n",
      "   0.12501812 0.12511821]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 這次讓我們將 padding mask 放入注意函式並觀察\n",
    "# 注意權重的變化\n",
    "mask = tf.squeeze(inp_mask, axis=1) # (batch_size, 1, seq_len_q)\n",
    "_, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 2), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.12485777, 0.12492246],\n",
       "        [0.12489326, 0.12496345],\n",
       "        [0.12477899, 0.12494591],\n",
       "        [0.12494876, 0.12491573],\n",
       "        [0.12494731, 0.12495708],\n",
       "        [0.1251301 , 0.12515235],\n",
       "        [0.1252261 , 0.1250457 ],\n",
       "        [0.12501812, 0.12511821]]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事實上也不完全是上句話的翻譯，\n",
    "# 因為我們在第一個維度還是把兩個句子都拿出來方便你比較\n",
    "attention_weights[:, :, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.04993161  0.02566061 -0.0146607   0.00574071]\n",
      "  [ 0.01592635 -0.02025861  0.00020757 -0.04203112]\n",
      "  [ 0.03664401  0.00267266  0.01699981 -0.04485391]\n",
      "  [ 0.02957764  0.02617563 -0.01678916  0.00045334]\n",
      "  [-0.03879989 -0.01194926  0.03493711 -0.04563121]\n",
      "  [-0.01690708  0.03204078  0.04683626  0.02910907]\n",
      "  [-0.02911331  0.02794905  0.01279471 -0.03680716]\n",
      "  [ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      "  [ 0.04868478 -0.0262701   0.02982837  0.04889109]\n",
      "  [ 0.04868478 -0.0262701   0.02982837  0.04889109]]\n",
      "\n",
      " [[ 0.04993161  0.02566061 -0.0146607   0.00574071]\n",
      "  [ 0.00005664 -0.03074064  0.04639968  0.0047685 ]\n",
      "  [ 0.03511107  0.04525493 -0.03638245 -0.01101079]\n",
      "  [-0.04905901  0.03428625  0.00740141 -0.01450055]\n",
      "  [ 0.02496466  0.03609515  0.0253484  -0.03687034]\n",
      "  [ 0.01206124 -0.02820102  0.03362811 -0.04972026]\n",
      "  [ 0.00390694  0.03242769  0.02957014 -0.0285221 ]\n",
      "  [-0.00034374  0.00425357  0.00814558 -0.02010503]\n",
      "  [-0.01690708  0.03204078  0.04683626  0.02910907]\n",
      "  [-0.02911331  0.02794905  0.01279471 -0.03680716]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "# 其遮罩為一個右上角的三角形\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask\", look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49970007 0.5003     0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.33315077 0.33332184 0.33352733 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.25014326 0.24983339 0.24994957 0.25007382 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.19964628 0.20010275 0.20006648 0.19974118 0.20044337 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16661058 0.16647813 0.16656879 0.1666195  0.16670413 0.16701886\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14270584 0.14282398 0.14284945 0.1427611  0.14299566 0.14285225\n",
      "   0.14301173 0.         0.         0.        ]\n",
      "  [0.12508176 0.12493542 0.12498348 0.12499884 0.12480913 0.12505396\n",
      "   0.12475887 0.12537858 0.         0.        ]\n",
      "  [0.11114639 0.11101635 0.11105905 0.1110727  0.11090413 0.11112168\n",
      "   0.11085947 0.11141013 0.11141013 0.        ]\n",
      "  [0.10000483 0.09988783 0.09992625 0.09993853 0.09978686 0.0999826\n",
      "   0.09974667 0.10024214 0.10024214 0.10024214]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49943003 0.50056994 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3336204  0.33253548 0.33384418 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24974783 0.2498701  0.24993283 0.2504492  0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.19999613 0.19982648 0.20003657 0.19991072 0.20023012 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16651267 0.16677019 0.16646032 0.16653857 0.16675167 0.1669665\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14278138 0.14276777 0.14281087 0.14286155 0.14296992 0.14286101\n",
      "   0.14294744 0.         0.         0.        ]\n",
      "  [0.12495741 0.12497579 0.12497292 0.12499849 0.12503463 0.12503818\n",
      "   0.12502576 0.12499688 0.         0.        ]\n",
      "  [0.11103684 0.11114057 0.11100205 0.11116983 0.11111403 0.11101251\n",
      "   0.11115178 0.11106349 0.11130893 0.        ]\n",
      "  [0.09988347 0.09991807 0.09994931 0.10009097 0.10003836 0.09999622\n",
      "   0.10005122 0.09998883 0.09998595 0.10009758]]], shape=(2, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 讓我們用目標語言（中文）的 batch\n",
    "# 來模擬 Decoder 處理的情況\n",
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0.49970007, 0.5003    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.49943003, 0.50056994, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[[-0.00560867  0.04747799 -0.02269538  0.02303809]\n",
      "  [ 0.02988834  0.00620367  0.03613586 -0.02179264]\n",
      "  [ 0.01707463  0.03969477  0.02194964 -0.03702695]\n",
      "  [ 0.01390957  0.0195005   0.03259501  0.02743271]\n",
      "  [ 0.02233395 -0.04510158 -0.00677161  0.04184886]\n",
      "  [-0.00289515 -0.02603846 -0.04859028  0.00019344]\n",
      "  [-0.02180409 -0.00259377  0.01643033 -0.01761655]\n",
      "  [-0.02180409 -0.00259377  0.01643033 -0.01761655]]\n",
      "\n",
      " [[-0.00560867  0.04747799 -0.02269538  0.02303809]\n",
      "  [ 0.00088333  0.02109436  0.0129119  -0.03082365]\n",
      "  [ 0.01852404  0.04630077 -0.02833566 -0.02577083]\n",
      "  [ 0.04514705  0.03137792 -0.04057758  0.04044752]\n",
      "  [ 0.04274544  0.02157653 -0.04907006  0.02949495]\n",
      "  [-0.02063758 -0.04873876 -0.03854288  0.020351  ]\n",
      "  [ 0.02233395 -0.04510158 -0.00677161  0.04184886]\n",
      "  [-0.00289515 -0.02603846 -0.04859028  0.00019344]]], shape=(2, 8, 4), dtype=float32)\n",
      "output: tf.Tensor(\n",
      "[[[[-0.00560867  0.04747799]\n",
      "   [ 0.02988834  0.00620367]\n",
      "   [ 0.01707463  0.03969477]\n",
      "   [ 0.01390957  0.0195005 ]\n",
      "   [ 0.02233395 -0.04510158]\n",
      "   [-0.00289515 -0.02603846]\n",
      "   [-0.02180409 -0.00259377]\n",
      "   [-0.02180409 -0.00259377]]\n",
      "\n",
      "  [[-0.02269538  0.02303809]\n",
      "   [ 0.03613586 -0.02179264]\n",
      "   [ 0.02194964 -0.03702695]\n",
      "   [ 0.03259501  0.02743271]\n",
      "   [-0.00677161  0.04184886]\n",
      "   [-0.04859028  0.00019344]\n",
      "   [ 0.01643033 -0.01761655]\n",
      "   [ 0.01643033 -0.01761655]]]\n",
      "\n",
      "\n",
      " [[[-0.00560867  0.04747799]\n",
      "   [ 0.00088333  0.02109436]\n",
      "   [ 0.01852404  0.04630077]\n",
      "   [ 0.04514705  0.03137792]\n",
      "   [ 0.04274544  0.02157653]\n",
      "   [-0.02063758 -0.04873876]\n",
      "   [ 0.02233395 -0.04510158]\n",
      "   [-0.00289515 -0.02603846]]\n",
      "\n",
      "  [[-0.02269538  0.02303809]\n",
      "   [ 0.0129119  -0.03082365]\n",
      "   [-0.02833566 -0.02577083]\n",
      "   [-0.04057758  0.04044752]\n",
      "   [-0.04907006  0.02949495]\n",
      "   [-0.03854288  0.020351  ]\n",
      "   [-0.00677161  0.04184886]\n",
      "   [-0.04859028  0.00019344]]]], shape=(2, 2, 8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "  # x.shape: (batch_size, seq_len, d_model)\n",
    "  batch_size = tf.shape(x)[0]\n",
    "  \n",
    "  # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "  assert d_model % num_heads == 0\n",
    "  depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "  \n",
    "  # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "  # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "  # (batch_size, seq_len, num_heads, depth)\n",
    "  reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "  \n",
    "  # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "  # (batch_size, num_heads, seq_len, depth)\n",
    "  output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "d_model = 4\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)  \n",
    "print(\"x:\", x)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # 在初始的時候建立一些必要參數\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "    self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "    \n",
    "    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n",
    "    \n",
    "    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n",
    "    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n",
    "  \n",
    "  # 這跟我們前面看過的函式有 87% 相似\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n",
    "    # 輸出會多一個 head 維度\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model)) \n",
    "    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    # 通過最後一個線性轉換\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
